{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "weird-ai.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bhMvYhue1Azi",
        "Dy7d-SUY9cE3",
        "C5BVbY-fFnop",
        "FI2NvlkoEOFr",
        "tKsQwCjX9hG5",
        "ESJrxw3F7Ygo",
        "AUGgxJzjYOw1",
        "PJT9WydO8bZB",
        "wfh6KBPaJBRt",
        "0BoaMdQ1GSYP",
        "P2t9Cd4d8zif",
        "AMLdfSu2GfTA",
        "1FBRWK4vG2t7",
        "2iZiO7py_z6g",
        "3I3FhY_h1RL0",
        "Gxk7rxv50APc",
        "CJ76XNmGziTT",
        "JGQ_wrxeZ2sr",
        "fDpxL_XpKCiD",
        "hfTgvG7cRuDe",
        "22iCwUrzR88y"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO3XChzP8XIcjyldBt0nmzf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markriedl/weirdai/blob/master/weird_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw3axI05TLQv"
      },
      "source": [
        "# Weird A.I Yankovic\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/h6xaopym08o977t/weird_ai_logo.JPG?dl=1\" alt=\"Weird A.I. Yankovic logo\" width=\"200px\"/>\n",
        "\n",
        "Weird A.I. Yankovic is a neural network based lyric generation system. Given a syllable and rhyme scheme, it attempts to generate new lyrics that fit that scheme.\n",
        "\n",
        "The intended use is to generate new lyrics for existing songs by feeding in the syllable and rhyme scheme for the song and then some contextualization information.\n",
        "\n",
        "It does not sing or match the lyrics to the music. You have to do that yourself. To make that easier, there are routines at the end for creating a karaoke video.\n",
        "\n",
        "This system was developed by [Mark Riedl](https://eilab.gatech.edu/mark-riedl). The work was performed independently of the Georgia Institute of Technology and released under MIT License.  \n",
        "\n",
        "To run: \n",
        "1. Make sure you have a GPU runtime. Use the Runtime >> Change Runtime Type menu and make sure \"GPU\" is selected.\n",
        "2. Select Runtime >> Run all\n",
        "3. Scroll down below until you find the web form.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhMvYhue1Azi"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy7d-SUY9cE3"
      },
      "source": [
        "## Install stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYHdNhfLe4QO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c006d1ab-6b43-4e11-ddce-58deb9c6bd73"
      },
      "source": [
        "!git clone https://github.com/markriedl/weirdai.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'weirdai' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLpu0qql7NaO"
      },
      "source": [
        "!pip3 install torch==1.7.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1rUxP0mjH4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad442a7e-f0a1-4aa2-ec40-2d69458e7f4f"
      },
      "source": [
        "!apt-get install festival espeak-ng\n",
        "!pip install phonemizer\n",
        "!pip install transformers==3.5.1\n",
        "!pip install pronouncing\n",
        "!pip install wordfreq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "espeak-ng is already the newest version (1.49.2+dfsg-1).\n",
            "festival is already the newest version (1:2.5.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n",
            "Requirement already satisfied: phonemizer in /usr/local/lib/python3.6/dist-packages (2.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from phonemizer) (0.16.0)\n",
            "Requirement already satisfied: segments in /usr/local/lib/python3.6/dist-packages (from phonemizer) (2.1.3)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.6/dist-packages (from phonemizer) (20.2.0)\n",
            "Requirement already satisfied: csvw>=1.5.6 in /usr/local/lib/python3.6/dist-packages (from segments->phonemizer) (1.8.0)\n",
            "Requirement already satisfied: clldutils>=1.7.3 in /usr/local/lib/python3.6/dist-packages (from segments->phonemizer) (3.5.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from segments->phonemizer) (2019.12.20)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.8.1)\n",
            "Requirement already satisfied: rfc3986 in /usr/local/lib/python3.6/dist-packages (from csvw>=1.5.6->segments->phonemizer) (1.4.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from csvw>=1.5.6->segments->phonemizer) (0.6.0)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from csvw>=1.5.6->segments->phonemizer) (3.0.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (4.2.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (0.8.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->csvw>=1.5.6->segments->phonemizer) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: pronouncing in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pronouncing) (0.4.4)\n",
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.6/dist-packages (2.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from wordfreq) (2019.12.20)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.6/dist-packages (from wordfreq) (1.0.0)\n",
            "Requirement already satisfied: langcodes>=2 in /usr/local/lib/python3.6/dist-packages (from wordfreq) (2.1.0)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.6/dist-packages (from langcodes>=2->wordfreq) (0.7.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4GRPR_pjmXH"
      },
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import pdb\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import sys\n",
        "from functools import reduce\n",
        "import string\n",
        "import random\n",
        "import shutil\n",
        "import functools\n",
        "from phonemizer.phonemize import phonemize\n",
        "from phonemizer.separator import Separator\n",
        "import pronouncing\n",
        "from wordfreq import word_frequency, top_n_list\n",
        "import IPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5BVbY-fFnop"
      },
      "source": [
        "## Set Up File Paths\n",
        "\n",
        "This is going to work faster if you mount your Google Drive and create a directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdqUJvJoAxo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680f379d-614e-4de5-8fdd-d62791bdbe62"
      },
      "source": [
        "### If you want to store rhyme dictionary and phone cache in your Google Drive\n",
        "### First mount your drive and create a \"weirdai\" directory\n",
        "### If false, will download the rhyme dictionary each time and recreate an empty phone cache each time\n",
        "MY_DRIVE = '/content/drive/My Drive'\n",
        "WEIRD_AI_PATH = os.path.join(MY_DRIVE, 'weirdai')\n",
        "NEAR_RHYME_DICT_FILENAME = 'near_rhymes20000'\n",
        "PHONE_CACHE_FILENAME = 'phone_cache'\n",
        "NEAR_RHYME_PATH = os.path.join('/content/', NEAR_RHYME_DICT_FILENAME)\n",
        "PHONE_CACHE_PATH = os.path.join('/content/', PHONE_CACHE_FILENAME)\n",
        "if not os.path.exists(MY_DRIVE):\n",
        "  print(\"Google Drive not mounted.\")\n",
        "elif not os.path.exists(WEIRD_AI_PATH):\n",
        "  try:\n",
        "    os.mkdir(WEIRD_AI_PATH)\n",
        "    print(\"Making new directory\", WEIRD_AI_PATH)\n",
        "  except:\n",
        "    print(\"Could not make cache directory in Google Drive.\")\n",
        "\n",
        "if os.path.exists(WEIRD_AI_PATH):\n",
        "  NEAR_RHYME_PATH = os.path.join(WEIRD_AI_PATH, NEAR_RHYME_DICT_FILENAME)\n",
        "  PHONE_CACHE_PATH = os.path.join(WEIRD_AI_PATH, PHONE_CACHE_FILENAME)\n",
        "  print(\"Setting near rhyme dictionary path to\", NEAR_RHYME_PATH)\n",
        "  print(\"Setting phoneme cache to\", PHONE_CACHE_PATH)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google Drive not mounted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI2NvlkoEOFr"
      },
      "source": [
        "## Global Parameters\n",
        "\n",
        "Some global parameters you can play with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6KUzGHRsSdG"
      },
      "source": [
        "### Top-k most frequent rhymes\n",
        "RHYME_K = 50                          \n",
        "# for top-k sampling of tokens from language models\n",
        "SAMPLE_K = 40                         \n",
        "# How many times to try a line\n",
        "NUM_TRIES = 6                         \n",
        "### Always pick the best scoring line?\n",
        "GREEDY_PICK_LINE = False\n",
        "### temperature for picking different possible options for lines. Set to 1 to be most permissive. \n",
        "### Set closer to zero to be more conservative and prefer the best scoring line more.\n",
        "PICK_LINE_TEMPERATURE = 0.1\n",
        "### Setting this too high can cause memory problems\n",
        "MAX_CONTEXT_LENGTH = 125"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKsQwCjX9hG5"
      },
      "source": [
        "## Rhyme dectection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_cH8BD98FKO"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWWwK6DPbURZ"
      },
      "source": [
        "### These are legit letters\n",
        "STR_LETTERS = set(string.ascii_letters + string.digits)\n",
        "\n",
        "### Is a string a word?\n",
        "def is_word(str):\n",
        "  str_set = set(str)\n",
        "  return len(str) > 0 and str_set.issubset(STR_LETTERS)\n",
        "\n",
        "### Is a letter a vowel?\n",
        "def is_vowel(letter):\n",
        "  return letter in ['a', 'e', 'i', 'o', 'u', 'y']\n",
        "\n",
        "### Is a letter a consonant?\n",
        "def is_consonant(letter):\n",
        "  return not is_vowel(letter)\n",
        "\n",
        "### Is a phoneme a vowel sound?\n",
        "### Pass in a phoneme (get this from Phonetic.phones_list)\n",
        "def is_vowel_sound(phoneme):\n",
        "  return is_vowel(phoneme[0])# and (is_vowel(phoneme[-1]) or phoneme[-1] in ['h', 'x', 'y'])\n",
        "\n",
        "### Is a phoneme a consonant sound?\n",
        "### Pass in a phoneme (get this from Phonetic.phones_list)\n",
        "def is_consonant_sound(phoneme):\n",
        "  return not is_vowel_sound(phoneme)\n",
        "\n",
        "### Return the number of vowel phonemes in a phoneneme list.\n",
        "### Pass in a list of phonemes (get this from Phonetic.phones_list)\n",
        "def num_vowel_phones(phone_list):\n",
        "  count = 0\n",
        "  for ph in phone_list:\n",
        "    if is_vowel(ph[0]):\n",
        "      count = count + 1\n",
        "  return count\n",
        "\n",
        "### For whatever reason, the first call to phonemize crashes because the API call fails.\n",
        "### This calls phonemize() once with a burner word just to get it out of the system\n",
        "def test_phonemize():\n",
        "  word = 'foobar'\n",
        "  try:\n",
        "    festival = phonemize(word, separator=Separator(phone='.', syllable='|', word=' ')).strip()\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    espeak = phonemize(word, backend='espeak', with_stress=True, separator=Separator(phone='.', syllable='', word=' ')).strip()\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "### Return the indicies of a character (ch) in a string (s)\n",
        "def find_char_indexes(s, ch):\n",
        "    return [i for i, ltr in enumerate(s) if ltr == ch]\n",
        "\n",
        "### Gets the phonemize API going\n",
        "test_phonemize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_KEiwRu8IfY"
      },
      "source": [
        "Get Phonemes and syllables for words.\n",
        "\n",
        "A Phonetic object stores several phonetic variations for a given word and knows the number of syllables for the word.\n",
        "\n",
        "The Phonetic constructor is given the word.\n",
        "\n",
        "Getting phonetic information is slow, so it caches its results in ``PHONE_CACHE``, which should be saved to disk to speed up processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErE8Rd_SqZB7"
      },
      "source": [
        "### Store phoneme information here for fast lookup\n",
        "PHONE_CACHE = {}  # Store results in cache for faster lookup\n",
        "\n",
        "### Phonetic class\n",
        "### word: orignal word\n",
        "class Phonetic():\n",
        "  def __init__(self, word):\n",
        "    global PHONE_CACHE\n",
        "    self.word = word                  # Remember the word\n",
        "    festival = None                   # festival results\n",
        "    espeak = None                     # espeak results\n",
        "    # If the word is in cache, use those results\n",
        "    if word in PHONE_CACHE:\n",
        "      festival, espeak = PHONE_CACHE[word]\n",
        "    else:\n",
        "      # API calls\n",
        "      festival = phonemize(word, separator=Separator(phone='.', syllable='|', word=' ')).strip()\n",
        "      espeak = phonemize(word, backend='espeak', with_stress=True, separator=Separator(phone='.', syllable='', word=' ')).strip()\n",
        "      # Store the results\n",
        "      PHONE_CACHE[word] = (festival, espeak)\n",
        "    # List of syllables. Each syllable is a list of phones\n",
        "    self.syllables_with_phones = [syl[:-1].split('.') for syl in festival.split('|')[:-1]]\n",
        "    # Just the syllables in raw form\n",
        "    self.syllables = festival.replace('.', '')\n",
        "    # Just the syllables in list form\n",
        "    self.syllables_list = self.syllables.split('|')[:-1]\n",
        "    # Just the phones in raw form\n",
        "    self.phones = festival.replace('|', '')\n",
        "    # Just the phones in list form\n",
        "    self.phones_list = self.phones.split('.')[:-1]\n",
        "    self.major_stress = 0       # The major stresses phone (index)\n",
        "    self.minor_stresses = []    # List of minor stressed phones (list of indices)\n",
        "    # Compute the major stress\n",
        "    major_stress_char_idxs = find_char_indexes(espeak, 'ˈ')\n",
        "    if len(major_stress_char_idxs) > 0:\n",
        "      stress_idx = espeak[:major_stress_char_idxs[0]].count('.')\n",
        "      if stress_idx < len(self.phones_list):\n",
        "        self.major_stress = stress_idx\n",
        "      else:\n",
        "        # Find the next earliest vowel phone\n",
        "        for i in range(len(self.phones_list)):\n",
        "          if is_vowel_sound(self.phones_list[len(self.phones_list)-i-1]):\n",
        "            self.major_stress = len(self.phones_list) - i - 1\n",
        "            break\n",
        "    # compute the minor stresses      \n",
        "    minor_stress_char_idxs = find_char_indexes(espeak, 'ˌ')\n",
        "    if len(minor_stress_char_idxs) > 0:\n",
        "      for char_idx in minor_stress_char_idxs:\n",
        "        stress_idx = espeak[:char_idx].count('.')\n",
        "        if stress_idx < len(self.phones_list):\n",
        "          self.minor_stresses.append(stress_idx)\n",
        "        else:\n",
        "          # Find the next earliest vowel phone\n",
        "          for i in range(len(self.phones_list)):\n",
        "            if is_vowel_sound(self.phones_list[len(self.phones_list)-i-1]):\n",
        "              self.minor_stresses.append(len(self.phones_list)-i-1)\n",
        "              break\n",
        "      # all stressed phones (list of indices)\n",
        "    self.all_stresses = sorted([self.major_stress] + self.minor_stresses[:])\n",
        "\n",
        "  def num_phones(self):\n",
        "    return len(self.phones_list)\n",
        "\n",
        "  def num_syllables(self):\n",
        "    return len(self.syllables_list)\n",
        "\n",
        "  def get_num_vowels(self):\n",
        "    return num_vowel_phones(self.phones_list)\n",
        "\n",
        "  def get_nth_vowel_phone(self, n=0):\n",
        "    count = -1\n",
        "    for i, ph in enumerate(self.phones_list):\n",
        "      if is_vowel_sound(ph):\n",
        "        count = count + 1\n",
        "      if count == n:\n",
        "        return ph, i\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def get_syllable_of_nth_phone(self, n):\n",
        "    count = -1\n",
        "    for i, syl in enumerate(self.syllables_list):\n",
        "      for ph in syl:\n",
        "        count = count + 1\n",
        "        if count == n:\n",
        "          return i\n",
        "    return None\n",
        "\n",
        "### Save the phone cache to disk\n",
        "def save_phone_cache(cache, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for key in list(cache.keys()):\n",
        "      festival, espeak = cache[key]\n",
        "      f.write(key + '\\t' + festival + '\\t' + espeak + '\\n')\n",
        "\n",
        "### Load the phone cache from disk\n",
        "def load_phone_cache(filename):\n",
        "  cache = {}\n",
        "  for line in open(filename, 'r'):\n",
        "    line = line.strip()\n",
        "    split_line = line.split('\\t')\n",
        "    if len(split_line) >= 3:\n",
        "      word, festival, espeak = split_line\n",
        "      cache[word] = (festival, espeak)\n",
        "  return cache\n",
        "\n",
        "def load_near_rhyme_dictionary(filename):\n",
        "  rhyme_dict = {}\n",
        "  for line in open(filename, 'r'):\n",
        "    line = line.split('\\t')\n",
        "    key = line[0].strip()\n",
        "    val = line[1].strip()\n",
        "    if key not in rhyme_dict:\n",
        "      rhyme_dict[key] = []\n",
        "    rhyme_dict[key].append(val)\n",
        "  return rhyme_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLDLu1D8Lzx"
      },
      "source": [
        "Detect perfect rhyme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK6w8bFqJOG3"
      },
      "source": [
        "### Return true if two words are perfect ryles of each other\n",
        "def perfect_rhyme(word1, word2):\n",
        "  phonetic1 = Phonetic(word1)\n",
        "  phonetic2 = Phonetic(word2)\n",
        "  stress1 = phonetic1.all_stresses[-1]\n",
        "  stress2 = phonetic2.all_stresses[-1]\n",
        "  if phonetic1.num_phones() - stress1 != phonetic2.num_phones() - stress2:\n",
        "    return False\n",
        "  for i in range(phonetic1.num_phones() - stress1):\n",
        "    phone1 = phonetic1.phones_list[i + stress1]\n",
        "    phone2 = phonetic2.phones_list[i + stress2]\n",
        "    if phone1 != phone2:\n",
        "      return False\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-cZYGuy8Qg3"
      },
      "source": [
        "Detect near rhyme at the phenome level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHlHLUJt62Jm"
      },
      "source": [
        "### Let's pretend that these phones are the same\n",
        "### (In addition all vowel phones that start with the same letter will be considered the same)\n",
        "NEAR_SETS = [['er', 'r'], \n",
        "             #['ih', 'eh'],\n",
        "             ['eh', 'ah'],\n",
        "             ['er', 'ax'],\n",
        "             ['ae', 'ey'],\n",
        "             ['t', 'f'],\n",
        "             ['b', 'k'],\n",
        "             ['p', 'z', 'th'],\n",
        "             ['s', 'st', 'sk'],\n",
        "             ['nt', 'ns'],\n",
        "             ['n', 'ng']\n",
        "             ]\n",
        "\n",
        "### These phonemes start with the same letter but should not be considered eqivalent\n",
        "FAR_SETS =[['aw', 'ay', 'ax'],\n",
        "           ['ax', 'ao']]\n",
        "\n",
        "### Determine if phonemes are a near match\n",
        "def near_match(phone1, phone2):\n",
        "  # Safety check\n",
        "  if (phone1 is None) or (phone2 is None) or (len(phone1) == 0 and len(phone2) > 0) or (len(phone2) == 0 and len(phone1) > 0):\n",
        "    return False\n",
        "  if phone1 == phone2:\n",
        "    # Phones are the same\n",
        "    return True\n",
        "  elif is_vowel_sound(phone1) and is_vowel_sound(phone2) and phone1[0] == phone2[0]:\n",
        "    # Vowel sounds start with the same letter\n",
        "    # Check that they are not in the same far set\n",
        "    for fs in FAR_SETS:\n",
        "      if phone1 in fs and phone2 in fs:\n",
        "        return False\n",
        "    return True\n",
        "  else:\n",
        "    # Check to see if the pair of phones are in the same near_set\n",
        "    for ns in NEAR_SETS:\n",
        "      if phone1 in ns and phone2 in ns:\n",
        "        return True\n",
        "  return False\n",
        "\n",
        "### Check if every phone in list 1 is a near match to corresponding phone in list 2\n",
        "def near_matches(phone_list1, phone_list2):\n",
        "  if len(phone_list1) != len(phone_list2):\n",
        "    return False\n",
        "  else:\n",
        "    for i in range(len(phone_list1)):\n",
        "      if not near_match(phone_list1[i], phone_list2[i]):\n",
        "        return False\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Haav2wIAY8cj"
      },
      "source": [
        "Detect near rhyme at the word level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0GdsPxb3Gtm"
      },
      "source": [
        "### If you turn this on, it will print information about the phoneme analysis being conducted\n",
        "### every time near_rhyme() is called. Not recommended\n",
        "VERBOSE = False\n",
        "\n",
        "### Verbose print allows for debug printing to be turned off\n",
        "def vprint(*args):\n",
        "  if VERBOSE:\n",
        "    print(' '.join([str(a) for a in args]))\n",
        "\n",
        "### Determine if word1 and word2 are near rhymes\n",
        "### if last_consonant is False, we won't check for near matches of the last consonant\n",
        "def near_rhyme(word1, word2, last_consonant = True):\n",
        "  if perfect_rhyme(word1, word2):\n",
        "    return True\n",
        "  phonetic1 = Phonetic(word1)\n",
        "  phonetic2 = Phonetic(word2)\n",
        "  stress1 = phonetic1.all_stresses[-1] # The last stressed phone\n",
        "  stress2 = phonetic2.all_stresses[-1] # The last stressed phone\n",
        "  vprint(phonetic1.syllables_with_phones, phonetic1.all_stresses)\n",
        "  vprint(phonetic2.syllables_with_phones, phonetic2.all_stresses)\n",
        "  \n",
        "  # Word = a v x w c\n",
        "  # a = prefix (not important)\n",
        "  # v = last stressed vowel\n",
        "  # x = any number of phones between v and w\n",
        "  # w = last vowel\n",
        "  # c = consonant after last vowel\n",
        "  \n",
        "  # Last stressed vowel (v)\n",
        "  v1_index = stress1\n",
        "  v2_index = stress2\n",
        "  v1 = phonetic1.phones_list[v1_index]\n",
        "  v2 = phonetic2.phones_list[v2_index]\n",
        "  vprint('v:', v1, v1_index, v2, v2_index)\n",
        "\n",
        "  # Last vowel (w)\n",
        "  w1, w1_index = phonetic1.get_nth_vowel_phone(phonetic1.get_num_vowels()-1)\n",
        "  w2, w2_index = phonetic2.get_nth_vowel_phone(phonetic2.get_num_vowels()-1)\n",
        "  vprint('w:', w1, w1_index, w2, w2_index)\n",
        "\n",
        "  # Consonants after last vowel (c)\n",
        "  c1_index = w1_index + 1\n",
        "  c1 = ''.join(phonetic1.phones_list[c1_index:phonetic1.num_phones()])\n",
        "  c2_index = w2_index + 1\n",
        "  c2 = ''.join(phonetic2.phones_list[c2_index:phonetic2.num_phones()])\n",
        "  vprint('c:', c1, c1_index, c2, c2_index)\n",
        "\n",
        "  # phones between v and w (x)\n",
        "  x1 = phonetic1.phones_list[v1_index+1:w1_index]\n",
        "  x2 = phonetic2.phones_list[v2_index+1:w2_index]\n",
        "  vprint('x:', x1, x2)\n",
        "\n",
        "  # p = first phone in x\n",
        "  # q = last phone in x\n",
        "  p1 = None\n",
        "  q1 = None\n",
        "  p2 = None\n",
        "  q2 = None\n",
        "  if len(x1) > 0:\n",
        "    p1 = x1[0]\n",
        "    q1 = x1[-1]\n",
        "  if len(x2) > 0:\n",
        "    p2 = x2[0]\n",
        "    q2 = x2[-1]\n",
        "  vprint('p,q:', p1, q1, p2, q2)\n",
        "\n",
        "  if not near_match(w1, w2):\n",
        "    vprint('w fail')\n",
        "    return False\n",
        "  elif not near_match(v1, v2):\n",
        "    vprint('v fail')\n",
        "    return False\n",
        "  elif False and len(c1) != len(c2) and (len(c1) > 1 or len(c2) > 1): \n",
        "    vprint('c not same length - fail')\n",
        "    return False\n",
        "  elif last_consonant and not near_match(c1, c2):\n",
        "    vprint('cs dont match - fail')\n",
        "    return False\n",
        "  elif len(x1) == 0 and len(x2) == 0:\n",
        "    vprint(\"no x - match\")\n",
        "    return True\n",
        "  elif len(x1) == 1 and len(x2) == 1 and near_match(x1[0], x2[0]):\n",
        "    vprint('single x - match')\n",
        "    return True\n",
        "  elif len(x1) > 0 and len(x2) > 0 and num_vowel_phones(x1) != num_vowel_phones(x2):\n",
        "    vprint('num vowel phones in x - fail')\n",
        "    return False\n",
        "  elif near_match(p1, p2) and near_match(q1, q2):\n",
        "    vprint('ps or qs - match')\n",
        "    return True\n",
        "  return False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESJrxw3F7Ygo"
      },
      "source": [
        "## Load Near Rhyme Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRw1MR7xvCIg"
      },
      "source": [
        "Download the near-rhyme dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3e2-iuTu8wS"
      },
      "source": [
        "if not os.path.exists(NEAR_RHYME_PATH): \n",
        "  !wget https://www.dropbox.com/s/8a800ivlp0uknic/near_rhymes20000.zip?dl=1 -O near_rhymes20000.zip\n",
        "  !unzip -o near_rhymes20000.zip\n",
        "if not os.path.exists(PHONE_CACHE_PATH):\n",
        "  !wget https://www.dropbox.com/s/i57hvmnlint7wj3/phone_cache.zip?dl=1 -O phone_cache.zip\n",
        "  !unzip -o phone_cache.zip\n",
        "if not os.path.exists(NEAR_RHYME_PATH):\n",
        "  shutil.copyfile(NEAR_RHYME_DICT_FILENAME, NEAR_RHYME_PATH)\n",
        "if not os.path.exists(PHONE_CACHE_PATH):\n",
        "  shutil.copyfile(PHONE_CACHE_FILENAME, PHONE_CACHE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnIVVEKmvGKv"
      },
      "source": [
        "Load the near-rhyme dictionary into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAdE5KR-vJcR"
      },
      "source": [
        "if os.path.exists(NEAR_RHYME_PATH):\n",
        "  NEAR_RHYME_DICT = load_near_rhyme_dictionary(NEAR_RHYME_PATH)\n",
        "\n",
        "if os.path.exists(PHONE_CACHE_PATH):\n",
        "  PHONE_CACHE = load_phone_cache(PHONE_CACHE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUGgxJzjYOw1"
      },
      "source": [
        "## Make a near-rhyme dictionary\n",
        "\n",
        "Searching for near-rhymes is expensive. Pre-compute near-rhymes and store them to disk.\n",
        "\n",
        "This only needs to be run once and the near-rhyme dictionary is made available for download. The only reason to run this function would be if you made changes to ``near_rhyme()``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwuHnrvJDVOZ"
      },
      "source": [
        "### Make a dictionary remembering whether words are near rhymes to each other.\n",
        "def make_near_rhyme_dictionary(top_n, filename, rhyme_dict=None):\n",
        "  global VERBOSE\n",
        "  VERBOSE = False   # Turn off debugging prints\n",
        "  near_rhymes = []\n",
        "  # Get top n most frequent words\n",
        "  top = top_n_list('en', top_n, wordlist='best')\n",
        "  for i, w1 in enumerate(top):\n",
        "    for j, w2 in enumerate(top):\n",
        "      if is_word(w1) and is_word(w2) and w1 != w2:\n",
        "        if rhyme_dict is None or w1 not in rhyme_dict or w2 not in rhyme_dict[w1]: \n",
        "          if near_rhyme(w1, w2):\n",
        "            print(\"MATCH\", i, j, w1, w2,)\n",
        "            near_rhymes.append((w1, w2))\n",
        "  if rhyme_dict is not None:\n",
        "    for key in list(rhyme_dict.keys()):\n",
        "      for val in rhyme_dict[key]:\n",
        "        near_rhymes.append((key, val))\n",
        "        near_rhymes.append((val, key))\n",
        "  with open(filename, 'w') as f:\n",
        "    for w1, w2 in near_rhymes:\n",
        "      f.write(w1 + '\\t' + w2 + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gPtx7krZCqx"
      },
      "source": [
        "Don't run this unless you want to rebuild the near-ryme dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgY9-p5NwRiF"
      },
      "source": [
        "build_near_rhyme_dictionary = False #@param {type:\"boolean\"}\n",
        "if build_near_rhyme_dictionary:\n",
        "  make_near_rhyme_dictionary(20000, NEAR_RHYME_PATH)\n",
        "  save_phone_cache(PHONE_CACHE, PHONE_CACHE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJT9WydO8bZB"
      },
      "source": [
        "## Load neural language models\n",
        "\n",
        "GPT-2 is probably the best generator we use, but it only goes forward. We only use GPT-2 when we are generating a line that has an unconstrained rhyme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlb6rJBD8Mgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81db6d5-1231-4077-ca74-df77705e2daf"
      },
      "source": [
        "GPT_TOKENIZER = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "GPT = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "GPT.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWuLSNXVa7pp"
      },
      "source": [
        "XLNET can generate forward and backward and fill in the middle of a sentence (sort of, we have to force it to operate this way)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UX0_jlq8k9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940711e5-f265-46c3-cc7f-c90d7867e8cd"
      },
      "source": [
        "XLNET_TOKENIZER = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "XLNET = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')\n",
        "XLNET.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLNetLMHeadModel(\n",
              "  (transformer): XLNetModel(\n",
              "    (word_embedding): Embedding(32000, 1024)\n",
              "    (layer): ModuleList(\n",
              "      (0): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (12): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (13): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (14): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (15): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (16): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (17): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (18): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (19): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (20): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (21): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (22): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (23): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_loss): Linear(in_features=1024, out_features=32000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPHI36TcbRu1"
      },
      "source": [
        "I was running into memory issues, so we only keep one neural language model in GPU memory at a time. This sets up ```@use_gpt``` and ```@use_xlnet``` function decorators. Put this before any function that will use either GPT-2 or XLNET. For example:\n",
        "```\n",
        "@use_gpt\n",
        "def my_cool_function(inputs):\n",
        "  outputs = GPT(inputs)\n",
        "  return outputs\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puqC81bR-qTU"
      },
      "source": [
        "### Are GPUs available?\n",
        "CUDA_AVAILABLE = torch.cuda.is_available()\n",
        "\n",
        "# Which model is in the GPU (string)\n",
        "MODEL_IN_GPU = None        \n",
        "# Which models are we using and what are their names?\n",
        "MODEL_HASH = {'gpt': GPT, 'xlnet': XLNET}\n",
        "\n",
        "def _prep_model(model_name):\n",
        "  global MODEL_IN_GPU\n",
        "  global MODELS_NOT_IN_GPU\n",
        "  if MODEL_IN_GPU != model_name:\n",
        "    # Unload the model in the gpu (if any)\n",
        "    if MODEL_IN_GPU is not None:\n",
        "      MODEL_HASH[MODEL_IN_GPU].to('cpu')\n",
        "      MODEL_IN_GPU = None\n",
        "    # Load the new model to gpu\n",
        "    if CUDA_AVAILABLE:\n",
        "      print(\"LOADING\", model_name)\n",
        "      MODEL_HASH[model_name].to('cuda')\n",
        "      MODEL_IN_GPU = model_name\n",
        "\n",
        "# For backward compatibility:\n",
        "def prep_gpt():\n",
        "  _prep_model('gpt')\n",
        "\n",
        "def prep_xlnet():\n",
        "  _prep_model('xlnet')\n",
        "\n",
        "# Decorators!\n",
        "def use_gpt(func):\n",
        "  def wrapper(*args, **kwargs):\n",
        "    _prep_model('gpt')\n",
        "    return func(*args, **kwargs)\n",
        "  return wrapper\n",
        "\n",
        "def use_xlnet(func):\n",
        "  def wrapper(*args, **kwargs):\n",
        "    _prep_model('xlnet')\n",
        "    return func(*args, **kwargs)\n",
        "  return wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpj25sSTOB9I"
      },
      "source": [
        "### This does top-k and top-p sampling from a list of logits. Borrowed from original GPT-2 code.\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfh6KBPaJBRt"
      },
      "source": [
        "##Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiNXarNSJAZx"
      },
      "source": [
        "### These are punctuation\n",
        "PUNCTUATION = ['.', ',', '-', '?', '!', ':', '_', '$', '%', '&', '#', '@', '*', '(', ')', '+', '=', '[', ']', '{', '}']\n",
        "### These are numbers\n",
        "NUMBERS = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n",
        "### Periods are important\n",
        "PERIOD = '.'\n",
        "BLANK = ' '\n",
        "\n",
        "### Is this string punctuation?\n",
        "def is_punctuation(s):\n",
        "  return len(set(s).intersection(set(PUNCTUATION))) > 0\n",
        "\n",
        "### Remove punctuation from string\n",
        "def remove_punctuation(s):\n",
        "  return ''.join(i for i in s if not i in PUNCTUATION) \n",
        "\n",
        "### How many syllables in this string?\n",
        "def get_syllables_for_line(line):\n",
        "  line = ''.join(list(filter(lambda c: c in STR_LETTERS or c == BLANK, line)))\n",
        "  count = 0\n",
        "  words = line.split()\n",
        "  for word in words:\n",
        "    word = word.strip()\n",
        "    if len(word) > 0 and is_word(word):\n",
        "      phonetic = Phonetic(word)\n",
        "      count = count + phonetic.num_syllables()\n",
        "  return count\n",
        "\n",
        "### Remove the prefix from the string (s)\n",
        "def remove_prefix(s, prefix):\n",
        "    rest = s[len(prefix):] if s.startswith(prefix) else s\n",
        "    return rest\n",
        "\n",
        "### The system considers whether to terminate a line after the line is generated.\n",
        "### So punctuation of a line could end up in the next line down.\n",
        "def fix_final_lines_punctuation(lines):\n",
        "  new_lines = []\n",
        "  for i, line in enumerate(lines):\n",
        "    if i > 0 and line[0] in PUNCTUATION:\n",
        "      new_lines[-1] = new_lines[-1] + line[0]\n",
        "      new_lines.append(line[1:].strip())\n",
        "    else: \n",
        "      new_lines.append(line.strip())\n",
        "  return new_lines\n",
        "\n",
        "def fix_final_lines_capitalization(lines):\n",
        "  new_lines = []\n",
        "  for line in lines:\n",
        "    new_line = line[0].upper() + line[1:].lower()\n",
        "    new_lines.append(new_line)\n",
        "  return new_lines\n",
        "\n",
        "### Figure out how to put two lines (strings) together.\n",
        "### If force_break is True, then put a sentence break (period) between the two.\n",
        "### Otherwise, try to figure out if there should be a period between.\n",
        "def merge_lines(l1, l2, force_break = False):\n",
        "  if len(l1) == 0:\n",
        "    return l2\n",
        "  elif len(l2) == 0 and force_break:\n",
        "    return l1 + PERIOD\n",
        "  elif len(l2) == 0:\n",
        "    return l1\n",
        "  elif not is_punctuation(l1[-1]) and force_break:\n",
        "    return l1 + PERIOD + BLANK + l2\n",
        "  else:\n",
        "    return l1 + BLANK + l2\n",
        "\n",
        "### Figure out which xlnet tokens are numbers\n",
        "def xlnet_number_tokens():\n",
        "  nums = []\n",
        "  for i in range(len(XLNET_TOKENIZER.get_vocab())):\n",
        "    s = XLNET_TOKENIZER.decode(i)\n",
        "    if len(s) > 0 and len(set(s).intersection(set(NUMBERS))) > 0: #s[0] in NUMBERS:\n",
        "      nums.append((i, s))\n",
        "  return nums\n",
        "\n",
        "### Figure out which gpt tokens are numbers\n",
        "def gpt_number_tokens():\n",
        "  nums = []\n",
        "  for i in range(len(GPT_TOKENIZER.get_vocab())):\n",
        "    s = GPT_TOKENIZER.decode(i)\n",
        "    if len(s) > 0 and len(set(s).intersection(set(NUMBERS))) > 0: #s[0] in NUMBERS:\n",
        "      nums.append((i, s))\n",
        "  return nums\n",
        "\n",
        "### Find all the numbers in xlnet and gpt vocabularies\n",
        "XLNET_NUMBER_TOKENS = list(map(lambda x: x[0], xlnet_number_tokens()))\n",
        "GPT_NUMBER_TOKENS = list(map(lambda x: x[0], gpt_number_tokens()))\n",
        "\n",
        "### Print lyrics \n",
        "def pretty_print(lines):\n",
        "  print('---------')\n",
        "  for line in lines:\n",
        "    print(line)\n",
        "  print('---------')\n",
        "\n",
        "EXCLAIMS = ['oh!', 'ah!', 'yeah!']\n",
        "\n",
        "## If we don't have enough syllables, add some ohs ahs and yeahs\n",
        "def add_filler(line, count, start):\n",
        "  prefix = line[0:start]\n",
        "  suffix = line[start:]\n",
        "  filler = []\n",
        "  for i in range(count):\n",
        "    filler.append(random.choice(EXCLAIMS))\n",
        "  return prefix + BLANK + BLANK.join(filler) + BLANK + suffix\n",
        "\n",
        "## We get weird punctuation. Remove it\n",
        "def remove_extra_punctuation(line):\n",
        "  idx = len(line)\n",
        "  for i in range(1, len(line)):\n",
        "    pos = len(line) - i\n",
        "    if line[pos] in PUNCTUATION:\n",
        "      idx = pos\n",
        "    else:\n",
        "      break\n",
        "  return line[0:idx]\n",
        "\n",
        "### Merge two dictionaries.\n",
        "### The dictionaries should contain lists as values.\n",
        "def merge_dicts(dict1, dict2):\n",
        "  if dict1 is None or dict2 is None:\n",
        "    return {}\n",
        "  new_dict = copy.deepcopy(dict1)\n",
        "  for key in list(dict2.keys()):\n",
        "    val_list = dict2[key]\n",
        "    if key not in new_dict:\n",
        "      new_dict[key] = []\n",
        "    new_dict[key] += val_list\n",
        "  return new_dict\n",
        "\n",
        "### Count how many times each word occurs in a line.\n",
        "def word_counts(line):\n",
        "  counts = {}\n",
        "  new_line = ''.join(list(filter(lambda c: c not in PUNCTUATION, line)))\n",
        "  for word in new_line.split():\n",
        "    if word not in counts:\n",
        "      counts[word] = 0\n",
        "    counts[word] += 1\n",
        "  return counts\n",
        "\n",
        "### Get rid of any lines that uses a word more than once\n",
        "def filter_lines(lines):\n",
        "  return list(filter(lambda line: max(word_counts(line).values()) <= 1, lines))\n",
        "\n",
        "### Make sure our lines don't get too long\n",
        "def crop_line(line):\n",
        "  enc = GPT_TOKENIZER.encode(line)\n",
        "  if len(enc) > MAX_CONTEXT_LENGTH:\n",
        "    enc = enc[len(enc)-MAX_CONTEXT_LENGTH:]\n",
        "  dec = GPT_TOKENIZER.decode(enc)\n",
        "  return dec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BoaMdQ1GSYP"
      },
      "source": [
        "##Post-processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPo-jcPTs2wr"
      },
      "source": [
        "\n",
        "Provide a specification hash where keys are line numbers and the values are ```(pre, post)``` such that ```pre``` is a string (or None) that should be prepended to the line and ```post``` is a string (or none) that should be appended to the line.\n",
        "\n",
        "```Pre``` and ```post``` can include special commands in brackets to do complex post-processing. Currently the commans supported are:\n",
        "\n",
        "- ```{repeat n}``` to repeat the last n syllables in the line (or \"all\").\n",
        "- ```{frepeat n}``` to repeat the first n syllables in the line (or \"all\")\n",
        "- ```{copy n}``` to copy the nth line and insert it directly after the line indicated by the key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73NX2UuQSKrF"
      },
      "source": [
        "def repeat_fn(line, all_lines, line_number, spec, args):\n",
        "  target_syllables = args[0]\n",
        "  ref_line = all_lines[line_number]\n",
        "  ref_line = ''.join(c for c in ref_line if not c in PUNCTUATION) \n",
        "  if target_syllables == 'all':\n",
        "    return ref_line, all_lines, line_number, spec\n",
        "  else:\n",
        "    ref_line_words = ref_line.split()\n",
        "    syllable_count = 0\n",
        "    picked_words = []\n",
        "    for word in reversed(ref_line_words):\n",
        "      num_syllables = get_syllables_for_line(word)\n",
        "      syllable_count = syllable_count + num_syllables\n",
        "      picked_words.append(word)\n",
        "      if syllable_count >= target_syllables:\n",
        "        break\n",
        "    return BLANK.join(reversed(picked_words)), all_lines, line_number, spec\n",
        "\n",
        "def frepeat_fn(line, all_lines, line_number, spec, args):\n",
        "  target_syllables = args[0]\n",
        "  ref_line = all_lines[line_number]\n",
        "  ref_line = ''.join(c for c in ref_line if not c in PUNCTUATION) \n",
        "  if target_syllables == 'all':\n",
        "    return ref_line, all_lines, line_number, spec\n",
        "  else:\n",
        "    ref_line_words = ref_line.split()\n",
        "    syllable_count = 0\n",
        "    picked_words = []\n",
        "    for word in ref_line_words:\n",
        "      num_syllables = get_syllables_for_line(word)\n",
        "      syllable_count = syllable_count + num_syllables\n",
        "      picked_words.append(word)\n",
        "      if syllable_count >= target_syllables:\n",
        "        break\n",
        "    return BLANK.join(picked_words), all_lines, line_number, spec\n",
        "\n",
        "def copy_fn(line, all_lines, line_number, spec, args):\n",
        "  new_spec = {}\n",
        "  line_to_copy = args[0]\n",
        "  copied_line = all_lines[line_to_copy]\n",
        "  new_all_lines = all_lines[0:line_number+1] + [copied_line] + all_lines[line_number+1:]\n",
        "  for key in list(spec.keys()):\n",
        "    val = spec[key]\n",
        "    if key > line_number:\n",
        "      new_spec[key+1] = val\n",
        "    else:\n",
        "      new_spec[key] = val\n",
        "  return '', new_all_lines, line_number, new_spec\n",
        "\n",
        "PARSE_FUNCTIONS = {'repeat': repeat_fn, \"copy\": copy_fn, 'frepeat': frepeat_fn}\n",
        "\n",
        "def parse(line, all_lines, line_number, spec):\n",
        "  done = False\n",
        "  while not done:\n",
        "    match = re.search(r'\\{([\\w]+)[ ]*([\\w, ]*)\\}', line)\n",
        "    if match is None:\n",
        "      done = True\n",
        "    else:\n",
        "      func = match.groups()[0]\n",
        "      args = eval(match.groups()[1])\n",
        "      if not isinstance(args, tuple):\n",
        "        args = tuple([args])\n",
        "      pre = line[0:match.start()]\n",
        "      mid, all_lines, line_number, spec = PARSE_FUNCTIONS[func](line, all_lines, line_number, spec, args)\n",
        "      post = line[match.end():]\n",
        "      line = pre + mid + post\n",
        "  return line, all_lines, line_number, spec\n",
        "\n",
        "### Add post-processing information to each line\n",
        "def post_process_lines(lines, spec):\n",
        "  new_lines = []\n",
        "  i = 0\n",
        "  while i < len(lines):\n",
        "    line = lines[i]\n",
        "    if i in spec:\n",
        "      pre, post = spec[i]\n",
        "      if pre is not None:\n",
        "        pre, lines, i, spec = parse(pre, lines, i, spec) if pre is not None else ''\n",
        "        line = lines[i]\n",
        "      else:\n",
        "        pre = ''\n",
        "      if post is not None:\n",
        "        post, lines, i, spec = parse(post, lines, i, spec) if post is not None else ''\n",
        "        line = lines[i]\n",
        "      else:\n",
        "        post = ''\n",
        "      new_lines.append(pre + line + post)\n",
        "    else:\n",
        "      new_lines.append(line)\n",
        "    i = i +1\n",
        "  return new_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2t9Cd4d8zif"
      },
      "source": [
        "## Picking Rhymes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SpMvUWFrqgC"
      },
      "source": [
        " ### Pick the word from the words list that is most similar to the context according to BERT\n",
        " @use_gpt\n",
        " def pick_similar(context, words, history = []): \n",
        "  words = words[:]\n",
        "  token_hash = {}\n",
        "  for word in words:\n",
        "    token_hash[tuple(GPT_TOKENIZER.encode(word))] = None\n",
        "  #prep_gpt()\n",
        "  context_tokens = GPT_TOKENIZER.encode(context) \n",
        "  prompt = torch.tensor([context_tokens]) # context put into the right shape\n",
        "  prompt = prompt.to('cuda') if CUDA_AVAILABLE else prompt\n",
        "  past = None \n",
        "  beams = []\n",
        "  for i in range(10):\n",
        "    current_beam = []\n",
        "    for j in range(20):\n",
        "      # Generate\n",
        "      output, new_past = GPT(prompt, past=past)\n",
        "      # Top k filter: there are k real numbers and the rest are -inf\n",
        "      logits = output[0][0][:]\n",
        "      tokens = torch.multinomial(F.softmax(logits), 10)\n",
        "      #token = tokens[0]\n",
        "      for tok in tokens.tolist():\n",
        "        word = GPT_TOKENIZER.decode(tok)\n",
        "        if not is_punctuation(word):\n",
        "          current_beam.append((tok, F.softmax(logits)))\n",
        "          prompt = torch.tensor([tok]).unsqueeze(0)\n",
        "          prompt = prompt.to('cuda')\n",
        "          past = new_past\n",
        "          break\n",
        "      #if token == 13:\n",
        "      #  break\n",
        "      #else:\n",
        "      #  current_beam.append((token, logits))\n",
        "      #  prompt = torch.tensor([token]).unsqueeze(0)\n",
        "      #  prompt = prompt.to('cuda')\n",
        "      #  past = new_past\n",
        "    beams.append(current_beam)\n",
        "  # ASSERT: we have 10 beams of 20 tokens or less\n",
        "  '''\n",
        "  for b in beams:\n",
        "    x = []\n",
        "    for tok, ls in b:\n",
        "      x.append(tok)\n",
        "    print('---')\n",
        "    print(GPT_TOKENIZER.decode(x))\n",
        "  '''\n",
        "  for key in list(token_hash.keys()):\n",
        "    sum = 0\n",
        "    for beam in beams:\n",
        "      for i in range(len(beam)):\n",
        "        for j, tok in enumerate(key):\n",
        "          if i+j < len(beam):\n",
        "            logits = beam[i+j][1]\n",
        "            sum = sum + logits[tok]\n",
        "        if len(key) > 1:\n",
        "          sum = sum / len(key)\n",
        "      #if len(beam) > 1:\n",
        "      #  sum = sum / len(beam)\n",
        "    if token_hash[key] is None:\n",
        "      token_hash[key] = sum\n",
        "    else:\n",
        "      token_hash[key] = max(token_hash[key], sum)\n",
        "  # ASSERT: Token_hash populated with good values\n",
        "  vals = torch.stack(list(token_hash.values()))\n",
        "  #soft = F.softmax(vals)\n",
        "  #mask = soft > 0.0\n",
        "  #num_nonzero = torch.sum(mask.int())\n",
        "  vals = vals + -vals.min()\n",
        "  #vals = F.softmax(vals)\n",
        "  #vals = -vals\n",
        "  #topk = torch.multinomial(soft, min(10, len(words), num_nonzero.item()), replacement=False).tolist()\n",
        "  final_pick = None\n",
        "  if vals.sum().item() > 0:\n",
        "    topk = torch.multinomial(vals, vals.size()[0], replacement=False).tolist()\n",
        "    print(\"Picking from:\", list(map(lambda x: words[x], topk)), \"given history\", history)\n",
        "    #pdb.set_trace()\n",
        "    for idx in topk:\n",
        "      if words[idx] not in history:\n",
        "        final_pick = words[idx]\n",
        "        break\n",
        "  if final_pick is None and len(words) >= 1:\n",
        "    final_pick = random.choice(words)\n",
        "  if final_pick is None:\n",
        "    # Uh oh we didn't find anything, probably because all the hits were in history\n",
        "    sorted_words = sorted(words, key=lambda w: token_hash[tuple(GPT_TOKENIZER.encode(w))], reverse=True)\n",
        "    print(\"Second attempt\", sorted_words, \"history\", history)\n",
        "    final_pick = None\n",
        "    for w in sorted_words:\n",
        "      if w not in history:\n",
        "        final_pick = w\n",
        "        break\n",
        "    if final_pick is None:\n",
        "      weights = list(map(lambda w: -token_hash[tuple(GPT_TOKENIZER.encode(w))], sorted_words))\n",
        "      print(\"Third attempt\", sorted_words, \"history\", history)\n",
        "      final_pick = random.choices(sorted_words, weights)[0]\n",
        "  print(\"Picking\", final_pick)\n",
        "  return final_pick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rPcqa0OHA8y"
      },
      "source": [
        "END_NO_WORDS = ['and', 'to', 'the', 'a', 'with', 'an', 'of', 'as', 'if', 'is', 'for', 'or', 'nor', 'in', 'we', 'my', 'from', 'that', 'your']\n",
        "\n",
        "NO_RHYME_WORDS = ['francoise', \"l'enfant\", 'un', 'vanhove', \"suhud\",\n",
        "                  're', 'le', 'tao', 'mao', 'lao', 'petr', 'chas', 'rao', 'raby', 'rabey', \n",
        "                  'ia', 'co', 'uk', 'a', 'i', 'the', 'nba', 'dna', 'fda', 'gta', \"c'est\", 'les', 'tor', 'dvd', 'labov',\n",
        "                  'stumm', 'yu', 'du', 'mon', 'aka', 'de', 'ne', 'que', 'ga', 'si', 'ap', 'ta', 'und', 'des',\n",
        "                  'wa', 'ka', 'ra', 'ba', 'da', 'aa', 'ca', 'va', 'ja', 'sa', 'fa', 'ou', 'eu', 'tu', 'su', 'fu', 'wu', 'dui', 'der',\n",
        "                  'ib', 'something', 'jesus', 'twentysomething', 'schaab', 'soo', 'grier', \"guangdong\"] + END_NO_WORDS\n",
        "### Remove some words from rhyme dictionary\n",
        "def filter_rhyme(word):\n",
        "  return (PERIOD not in word) and \\\n",
        "          ('-' not in word) and \\\n",
        "          (len(word) > 1 or word == 'a' or word == 'I' or word == 'i') and \\\n",
        "          (word.lower() not in NO_RHYME_WORDS)\n",
        "\n",
        "### Uniformly pick amongst top k most frequent perfect rhymes\n",
        "def pick_perfect_rhyme(word, context = None, history=[]):\n",
        "  # Remove unwanted letters\n",
        "  word = ''.join(list(filter(lambda c: c in STR_LETTERS, word)))\n",
        "  # Get rhymes\n",
        "  rhymes = pronouncing.rhymes(word)\n",
        "  # Remove unwanted words\n",
        "  rhymes = list(filter(lambda w: filter_rhyme(w), rhymes))\n",
        "  # If there aren't any rhymes, return the current word\n",
        "  if len(rhymes) == 0:\n",
        "    return word\n",
        "  else:\n",
        "    # Make sure k is smaller than total number of rhymes available\n",
        "    k = min(RHYME_K, len(rhymes))\n",
        "    # Get word frequency of rhymes\n",
        "    probs = list(map(lambda r: word_frequency(r, 'en'), rhymes))\n",
        "    probs_tensor = torch.tensor(probs)\n",
        "    # Get top k\n",
        "    vals, idxs = torch.topk(probs_tensor, k)\n",
        "    if context is not None and len(context) > 0:\n",
        "      candidates = [rhymes[x] for x in idxs.tolist()]\n",
        "      pick = pick_similar(context, candidates, history)\n",
        "      return pick\n",
        "    else:\n",
        "      # Pick uniformly\n",
        "      r = random.randint(0, k-1)\n",
        "      return rhymes[idxs[r].item()]\n",
        "\n",
        "### Pick best near rhyme from the near rhyme dictionary\n",
        "def pick_near_rhyme(word, context = None, history = []):\n",
        "  global NEAR_RHYME_DICT\n",
        "  if NEAR_RHYME_DICT is None:\n",
        "    NEAR_RHYME_DICT = load_near_rhyme_dictionary(NEAR_RHYME_PATH)\n",
        "  # Remove unwanted letters\n",
        "  word = ''.join(list(filter(lambda c: c in STR_LETTERS, word)))\n",
        "  # Get rhymes\n",
        "  near_rhymes = []\n",
        "  #rhymes = []\n",
        "  if word in NEAR_RHYME_DICT:\n",
        "    near_rhymes = NEAR_RHYME_DICT[word]\n",
        "  if len(near_rhymes) > 0:\n",
        "    rhymes = near_rhymes\n",
        "  #else:\n",
        "  #  rhymes = pronouncing.rhymes(word)\n",
        "  rhymes = list(set(near_rhymes + pronouncing.rhymes(word)))\n",
        "  # Remove unwanted words\n",
        "  rhymes = list(filter(lambda w: filter_rhyme(w), rhymes))\n",
        "  # If there aren't any rhymes, return the current word\n",
        "  if len(rhymes) == 0:\n",
        "    return word\n",
        "  else:\n",
        "    # Make sure k is smaller than total number of rhymes available\n",
        "    k = min(RHYME_K, len(rhymes))\n",
        "    # Get word frequency of rhymes\n",
        "    probs = list(map(lambda r: word_frequency(r, 'en'), rhymes))\n",
        "    probs_tensor = torch.tensor(probs)\n",
        "    # Get top k\n",
        "    vals, idxs = torch.topk(probs_tensor, k)\n",
        "    if context is not None and len(context) > 0:\n",
        "      candidates = [rhymes[x] for x in idxs.tolist()]\n",
        "      pick = pick_similar(context, candidates, history)\n",
        "      return pick\n",
        "    else:\n",
        "      # Pick uniformly\n",
        "      r = random.randint(0, k-1)\n",
        "      return rhymes[idxs[r].item()]\n",
        "\n",
        "### Pick rhyme, redirects to pick_perfect_rhyme or pick_near_rhyme\n",
        "def pick_rhyme(word, perfect=True, context=None, history=[]):\n",
        "  if perfect:\n",
        "    return pick_perfect_rhyme(word, context=context, history=history)\n",
        "  else:\n",
        "    return pick_near_rhyme(word, context=context, history=history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMLdfSu2GfTA"
      },
      "source": [
        "## Lyrics Scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xB0K0RMGsjY"
      },
      "source": [
        "Compute a score for a segment of lyrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiWmyvPbGp45"
      },
      "source": [
        "LOSS = torch.nn.CrossEntropyLoss(reduction='sum')  \n",
        "\n",
        "def interactive_pick_line(lines):\n",
        "  result = None\n",
        "  print(\"GENERATED CANDIDATES:\")\n",
        "  for i, line in enumerate(lines):\n",
        "    print('[' + str(i) + '] ' + line)\n",
        "  inp = input(\"CHOOSE BY NUMBER (0-\" + str(len(lines)-1) + \") OR WRITE YOUR OWN: \")\n",
        "  try:\n",
        "    idx = int(inp)\n",
        "    result = lines[idx]\n",
        "  except ValueError:\n",
        "    result = inp\n",
        "  return result, INF\n",
        "\n",
        "\n",
        "### Score a sentence, lower is better (cross entropy)\n",
        "@use_gpt\n",
        "def score_sentence(sentence):\n",
        "  #encode sentence\n",
        "  prompt = GPT_TOKENIZER.encode(sentence)\n",
        "  # Set up x and y as shifted input\n",
        "  x = torch.tensor([prompt[:-1]])\n",
        "  y = torch.tensor(prompt[1:])\n",
        "  if CUDA_AVAILABLE:\n",
        "    x = x.to('cuda')\n",
        "    y = y.to('cuda')\n",
        "  #prep_gpt()  \n",
        "  # Measure the logits for loss\n",
        "  output, new_past = GPT(x, past=None)\n",
        "  score = LOSS(output[0], y)\n",
        "  return score\n",
        "\n",
        "\n",
        "### Pick the best line. Run them all through the scoring function and pick the smallest\n",
        "def pick_best_line(lines, context, history = [], interactive=False):\n",
        "  if interactive:\n",
        "    return interactive_pick_line(lines)\n",
        "  else:\n",
        "    # Remove duplicates\n",
        "    modified_history = list(map(lambda s: remove_punctuation(s).encode('ascii', 'ignore').lower(), history))\n",
        "    filtered_lines = list(filter(lambda s: remove_punctuation(s).encode('ascii', 'ignore').lower() not in modified_history, lines))\n",
        "    lines = filtered_lines if len(filtered_lines) > 0 else lines\n",
        "    # Score sentences\n",
        "    scores = list(map(lambda l:score_sentence(merge_lines(context, l)), lines))\n",
        "    scores_tensor = torch.tensor(scores, dtype=torch.float)\n",
        "    #for i, line in enumerate(lines):\n",
        "    #  print(\"SCORE\", scores[i].item(), line)\n",
        "    # Get smallest\n",
        "    if len(lines) == 1:\n",
        "      return lines[0], scores[0].item()\n",
        "    elif GREEDY_PICK_LINE:\n",
        "      vals, idxs = torch.topk(scores_tensor, 1, largest=False)\n",
        "      idx = idxs[0].item()\n",
        "      return lines[idx], vals[0].item()\n",
        "    else:\n",
        "      try:\n",
        "        shifted = (scores_tensor-min(scores_tensor))\n",
        "        flipped = (max(shifted) - shifted).div(max(shifted))\n",
        "        exped = flipped.div(PICK_LINE_TEMPERATURE).exp()\n",
        "        idxs = torch.multinomial(exped, 1)\n",
        "        idx = idxs[0].item()\n",
        "      except:\n",
        "        # Probably a div by zero\n",
        "        # Usually caused when all the sentences are the same and thus have the same scores\n",
        "        # Which means shifted is a tensor of 0s\n",
        "        idx = 0\n",
        "      print(\"PICK\", lines[idx])\n",
        "      return lines[idx], scores[idx].item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FBRWK4vG2t7"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF-qFh_083UY"
      },
      "source": [
        "Fill in masks between context and a rhyme (ending)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RibFtz0Dfzg_"
      },
      "source": [
        "MASK_IDX = 6                          # MASK is token id 6\n",
        "PERIOD_IDX = 9                        # Period is token id 9\n",
        "# Things I don't want generated\n",
        "XLNET_NO_TOKENS = [6, 7, 8, 0, 1, 2, 3, 4, 5,\n",
        "                   10, 11, 12, 13, 14, 15, 16, 2055, 6490, 26,\n",
        "                   97, 167, 225, 4145, 3158, 17115, 22891, 17666, 4538, 1926, 22788, 2780, 19348, 16011] + XLNET_NUMBER_TOKENS\n",
        "NINF = -float('Inf')                  # Negative infinity\n",
        "INF = float('Inf')                    # Positive infinity\n",
        "\n",
        "### Given a prompt, with one or more '<mask>' in it, fill the masks in with XLNET \n",
        "### Can go forward or backward. Backward seems to work better?\n",
        "@use_xlnet\n",
        "def fill_line(prompt, backward=False):\n",
        "  generated_tokens = []\n",
        "  # Convert prompt into tokens\n",
        "  input_ids = torch.tensor(XLNET_TOKENIZER.encode(prompt, add_special_tokens=False)).unsqueeze(0)  \n",
        "  # mask out the places we want to predict\n",
        "  perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "  masked = input_ids == MASK_IDX\n",
        "  perm_mask = perm_mask + masked\n",
        "  # The places we want to predict are...\n",
        "  predicts = torch.nonzero(masked[0]).tolist()\n",
        "  if backward:\n",
        "    predicts = list(reversed(predicts))\n",
        "  # Set up a diagonal where we want to predict, dim=0 is batch, dim=1 is each prediction\n",
        "  target_mapping = torch.zeros((1, len(predicts), input_ids.shape[1]), dtype=torch.float)  \n",
        "  for n, p in enumerate(predicts):\n",
        "    target_mapping[0][n][p] = 1.0\n",
        "  #prep_xlnet()\n",
        "  if CUDA_AVAILABLE:\n",
        "    input_ids = input_ids.to('cuda')\n",
        "    perm_mask = perm_mask.to('cuda')\n",
        "    target_mapping = target_mapping.to('cuda')\n",
        "\n",
        "  # Fill one mask at a time until there are no places to fill (predicts is empty)\n",
        "  while len(predicts) > 0:\n",
        "    # Predict everything, but ignore all but the first prediction\n",
        "    outputs = XLNET(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n",
        "    next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n",
        "    # Filter to top-k\n",
        "    logits = top_k_top_p_filtering(next_token_logits[0][0], top_k=SAMPLE_K)\n",
        "    # Sample from top-k\n",
        "    samples = torch.multinomial(F.softmax(logits), SAMPLE_K)\n",
        "    # Make sure we didn't predict a repetition and not in NO_TOKENS\n",
        "    pos = torch.nonzero(target_mapping[0][0]).item()\n",
        "    previous_token = input_ids[0][pos-1].item()\n",
        "    next_token = input_ids[0][pos+1].item()\n",
        "    token = None\n",
        "    for i in range(SAMPLE_K):\n",
        "      tok = samples[i].item()\n",
        "      if tok != previous_token and tok != next_token and tok not in XLNET_NO_TOKENS:\n",
        "        # Avoid ALL punctuation\n",
        "        if not is_punctuation(XLNET_TOKENIZER.decode(tok)):\n",
        "          # avoid repeat tokens\n",
        "          word = XLNET_TOKENIZER.decode(tok)\n",
        "          if word.lower() not in XLNET_TOKENIZER.decode(generated_tokens).lower():\n",
        "            # avoid ending with weird words\n",
        "            #if word.lower() in END_NO_WORDS:\n",
        "            #  print('--', word.lower(), len(predicts))\n",
        "            if word.lower() not in END_NO_WORDS or len(predicts) > 1:\n",
        "              token = tok\n",
        "              generated_tokens.append(tok)\n",
        "              break\n",
        "    if token is None:\n",
        "      token = samples[0].item()\n",
        "      generated_tokens.append(token)\n",
        "    # insert the token into the input\n",
        "    input_ids[0][pos] = token\n",
        "    #print(XLNET_TOKENIZER.decode(input_ids[0]))\n",
        "    # Mask out everything that needs to be masked\n",
        "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "    perm_mask = perm_mask.to('cuda')\n",
        "    masked = input_ids == 6\n",
        "    perm_mask = perm_mask + masked\n",
        "    # Update predicts, should be one less\n",
        "    predicts = torch.nonzero(masked[0]).tolist()\n",
        "    if backward:\n",
        "      predicts = list(reversed(predicts))\n",
        "    # Set up a diagonal where we want to predict, dim=0 is batch, dim=1 is each prediction\n",
        "    target_mapping = torch.zeros((1, len(predicts), input_ids.shape[1]), dtype=torch.float)  \n",
        "    for n, p in enumerate(predicts):\n",
        "      target_mapping[0][n][p] = 1.0\n",
        "    target_mapping = target_mapping.to('cuda')\n",
        "  return XLNET_TOKENIZER.decode(input_ids[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3V-oq679irt"
      },
      "source": [
        "Generate a line the ends in a rhyme (XLNET)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZz0gibV3F4p"
      },
      "source": [
        "def generate_rhyme_line(rhyme, context, target_syllables, terminate_line=True):\n",
        "  good_tries = [] # runs with the exact number of syllables\n",
        "  bad_tries = {}  # Runs with fewer syllables (dict is num_syllables: list of lines)\n",
        "  # i is number of masks\n",
        "  # j is number of tries\n",
        "  for i in range(int(target_syllables*1.5)):\n",
        "    for j in range(NUM_TRIES):\n",
        "      ## Even tries have a period added at the end of the previous context\n",
        "      #if j % 2 == 0 and len(context) > 0 and context[-1] not in PUNCTUATION and not no_terminate_previous:\n",
        "      #  context_copy = context[:] + PERIOD\n",
        "      #else:\n",
        "      context_copy = context[:] + BLANK\n",
        "      # This fixes things up\n",
        "      context_copy = XLNET_TOKENIZER.decode(XLNET_TOKENIZER.encode(context_copy, add_special_tokens=False))\n",
        "      # Make masks\n",
        "      prompt = context_copy + BLANK + BLANK.join(['<mask>']*(i+1)) + BLANK + rhyme\n",
        "      if terminate_line or j % 2 == 1:\n",
        "        prompt = prompt + PERIOD\n",
        "      # Fill masks\n",
        "      filled_line = fill_line(prompt, backward=True)\n",
        "      # Figure out how many syllables we added\n",
        "      candidate = remove_prefix(filled_line, context_copy).strip() # the newly added line\n",
        "      line_syllable_count = get_syllables_for_line(candidate) # Number of syllables in newly added line\n",
        "      # Did we get a good run?\n",
        "      if line_syllable_count == target_syllables:\n",
        "        good_tries.append(candidate)\n",
        "        print(\"CANDIDATE\", candidate)\n",
        "      elif line_syllable_count < target_syllables:\n",
        "        # Bad tries are those that are too short. Store in dict by length\n",
        "        if line_syllable_count not in bad_tries:\n",
        "          bad_tries[line_syllable_count] = []\n",
        "        bad_tries[line_syllable_count].append(candidate)\n",
        "  return good_tries, bad_tries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueZ3qzhe9nbG"
      },
      "source": [
        "Generate an open-ended line (GPT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS-blkCc4Twh"
      },
      "source": [
        "GPT_NO_TOKENS = [59, 50256, 1, 6, 7, 8, 12, 14, 26, 58, 59, 60, 62, 90, 92, 2503, 3,\n",
        "                 1906, 14988, 26391, 4023, 338, 40578, 14231, 14036, 15466, 525] + GPT_NUMBER_TOKENS\n",
        "\n",
        "@use_gpt\n",
        "def generate_non_rhyme_line(context, target_syllables):\n",
        "  generated_tokens = [] # Tokens generated along the way\n",
        "  good_tries = [] # results with the correct number of syllables\n",
        "  past_syllables = get_syllables_for_line(context) # How many syllables in the context\n",
        "  #prep_gpt()\n",
        "  # j is number of tries\n",
        "  for j in range(NUM_TRIES):\n",
        "    new_syllables = 0 # How many new syllables were produced\n",
        "    # Odd tries add a period to the context\n",
        "    #if j % 2 == 0 and len(context) and context[-1] not in PUNCTUATION and not no_terminate_previous:\n",
        "    #  context_copy = context[:] + PERIOD\n",
        "    #else:\n",
        "    context_copy = context[:] + BLANK\n",
        "    # Encode the context\n",
        "    generated = GPT_TOKENIZER.encode(context_copy) # Used to collect up tokens\n",
        "    prompt = torch.tensor([generated]) # context put into the right shape\n",
        "    prompt = prompt.to('cuda') if CUDA_AVAILABLE else prompt\n",
        "    past = None # Initially we don't have any history\n",
        "    # Generate until we get enough syllables\n",
        "    previous_token = generated[-1] if len(generated) > 0 else None\n",
        "    count = 0\n",
        "    while new_syllables < target_syllables and count < 1000: # break loop if too many iterations\n",
        "      # Generate\n",
        "      output, new_past = GPT(prompt, past=past)\n",
        "      # Top k filter: there are k real numbers and the rest are -inf\n",
        "      logits = top_k_top_p_filtering(output[0][0], top_k=SAMPLE_K)\n",
        "      tokens = torch.multinomial(F.softmax(logits), SAMPLE_K)\n",
        "      # Pick the first one from the top k that doesn't produce too many syllables\n",
        "      for tok in tokens.tolist():\n",
        "        # How many syllables do we have total?\n",
        "        line_syllables = get_syllables_for_line(GPT_TOKENIZER.decode(generated + [tok]))\n",
        "        # Have we gone over? Or generated a NO_TOKEN?\n",
        "        if line_syllables <= target_syllables + past_syllables and tok not in GPT_NO_TOKENS:\n",
        "          # We are good\n",
        "          word = GPT_TOKENIZER.decode(tok) # The new word\n",
        "          # Don't allow ANY punctuation\n",
        "          if not is_punctuation(word):\n",
        "            if word.lower() not in GPT_TOKENIZER.decode(generated_tokens).lower():\n",
        "              generated_tokens.append(tok)\n",
        "              # Add new word to generated\n",
        "              generated.append(tok) \n",
        "              # Prep for the next run\n",
        "              prompt = torch.tensor([tok]).unsqueeze(0)\n",
        "              prompt = prompt.to('cuda')\n",
        "              past = new_past\n",
        "              new_syllables = line_syllables - past_syllables\n",
        "              break\n",
        "      count = count + 1\n",
        "    candidate = remove_prefix(GPT_TOKENIZER.decode(generated), context).strip()\n",
        "    print(\"CANDIDATE\", candidate)\n",
        "    good_tries.append(candidate)\n",
        "  return good_tries, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f9phyjZfNsv"
      },
      "source": [
        "Generate a line that is guaranteed to end in a period (using XLNET)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NgzbyQEMsLm"
      },
      "source": [
        "def generate_terminal_non_rhyme_line(context, target_syllables):\n",
        "  good_tries = [] # runs with the exact number of syllables\n",
        "  bad_tries = {}  # Runs with fewer syllables (dict is num_syllables: list of lines)\n",
        "  # i is number of masks\n",
        "  # j is number of tries\n",
        "  for i in range(int(target_syllables*1.5)):\n",
        "    for j in range(NUM_TRIES):\n",
        "      # Even tries have a period added at the end of the previous context\n",
        "      #if j % 2 == 0 and len(context) > 0 and context[-1] not in PUNCTUATION and not no_terminate_previous:\n",
        "      #  context_copy = context[:] + PERIOD\n",
        "      #else:\n",
        "      context_copy = context[:] + BLANK\n",
        "      # This fixes things up\n",
        "      context_copy = XLNET_TOKENIZER.decode(XLNET_TOKENIZER.encode(context_copy, add_special_tokens=False))\n",
        "      # Make masks\n",
        "      prompt = context_copy + BLANK + BLANK.join(['<mask>']*(i+1)) + PERIOD # Different from generate_rhyme_line\n",
        "      # Fill masks\n",
        "      filled_line = fill_line(prompt, backward=False) # Forward instead of backward\n",
        "      # Figure out how many syllables we added\n",
        "      candidate = remove_prefix(filled_line, context_copy).strip() # the newly added line\n",
        "      line_syllable_count = get_syllables_for_line(candidate) # Number of syllables in newly added line\n",
        "      # Did we get a good run?\n",
        "      if line_syllable_count == target_syllables:\n",
        "        good_tries.append(candidate)\n",
        "        print(\"CANDIDATE\", candidate)\n",
        "      elif line_syllable_count < target_syllables:\n",
        "        # Bad tries are those that are too short. Store in dict by length\n",
        "        if line_syllable_count not in bad_tries:\n",
        "          bad_tries[line_syllable_count] = []\n",
        "        bad_tries[line_syllable_count].append(candidate)\n",
        "  return good_tries, bad_tries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iZiO7py_z6g"
      },
      "source": [
        "##Main execution loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkFtgmOyztGY"
      },
      "source": [
        "def run(context, scheme, rhyme_dict, use_near_rhymes = False, post = None, \n",
        "        recontextualize = False, interactive = False):\n",
        "  ### SETUP #####################################\n",
        "  lines = []    # Store the lyrics lines\n",
        "  segments = [] # Store each segment separately\n",
        "  # Set up the context\n",
        "  context = context[:].strip()\n",
        "  original_context = context[:]\n",
        "  # Set up rhyme history\n",
        "  rhyme_history = [] # The history of words used for rhymes\n",
        "  # original context shouldn't have punctuation at the end, but the first prompt to the neural net should\n",
        "  if len(context) >0 and context[-1] not in PUNCTUATION:\n",
        "    context = context + PERIOD\n",
        "  if len(original_context) > 0 and original_context[-1] in PUNCTUATION:\n",
        "    original_context = original_context[:-1]\n",
        "  # copy the rhyme_dict because we will be adding to it\n",
        "  rhyme_dict = copy.deepcopy(rhyme_dict)\n",
        "  ### ITERATE THROUGH SCHEME #####################\n",
        "  # An entry can be tuples or lists of tuples. \n",
        "  # Easiest thing is to nest tuples into lists and treat everything the same\n",
        "  for n, entry in enumerate(scheme):\n",
        "    ### RECONTEXTUALIZE ##########################\n",
        "    # Insert the original context into the history/context at sentence breaks\n",
        "    if n > 0 and recontextualize:\n",
        "      last_period = context.rfind(PERIOD)\n",
        "      last_contextualization = context.rfind(original_context)\n",
        "      if last_period >= 0 and last_contextualization >= 0 and last_contextualization + len(original_context) - 1 != last_period - 1:\n",
        "        context = merge_lines(merge_lines(context[:last_period+1], original_context, force_break=True), context[last_period+1:], force_break=True)\n",
        "    ### ITERATE THROUGH LINE SPEC #################\n",
        "    current_line = []   # The line currently being worked on. May be made of several segments\n",
        "    # if line is a single segment, nest it\n",
        "    if isinstance(entry, tuple):\n",
        "      entry = [entry]\n",
        "    # A line has 1 or more tuples of the form (target_syllables, rhyme_index)\n",
        "    for segment_num, segment in enumerate(entry):\n",
        "      is_end_segment = (segment_num == len(entry) - 1)\n",
        "      target_syllables = segment[0]\n",
        "      rhyme_idx = segment[1]\n",
        "      cmd = None\n",
        "      goods = []    # lines with the right number of syllables\n",
        "      shorts = {}   # lines with too few syllables\n",
        "      new_context = None # The complete lyrics after a new line is added\n",
        "      new_syllables = 0 # The number of new syllables added to lyrics\n",
        "      terminate_segment = False\n",
        "      ### PARSE SPECIAL COMMANDS ##################\n",
        "      # We have commands to parse\n",
        "      if len(segment) > 2:\n",
        "        cmd = segment[2]\n",
        "        terminate_segment = (cmd == ':end')\n",
        "      print(\"LINE\", n, \"SEGMENT\", segment_num, '(end)' if is_end_segment else '', \"TARGET SYLLABLES\", target_syllables, \"RHYME INDEX\", rhyme_idx, \"COMMAND\", cmd)\n",
        "      # Check if we are filling in a partial line (when there is a rhyme or when there is a given verbatim string)\n",
        "      if isinstance(rhyme_idx, str) or rhyme_idx in rhyme_dict:\n",
        "        ### USE RHYME INDEX ########################\n",
        "        # We are filling a line\n",
        "        end_targets = [] # The line should end in this word (or words)\n",
        "        ### RHYME INDEX IS A STRING ####\n",
        "        if isinstance(rhyme_idx, str):\n",
        "          # Use a verbatim string\n",
        "          end_targets = [rhyme_idx] # rhyme_idx is actually a string\n",
        "          # If target_syllables < 0 then set it to the exact number of syllables in the rhyme_idx string\n",
        "          if target_syllables < 0:\n",
        "            target_syllables = get_syllables_for_line(rhyme_idx)\n",
        "        ### PICK A RHYME WORD ####\n",
        "        else:\n",
        "          # Get some rhyming words\n",
        "          # but first fix the dictionary if there is a single word instead of a list\n",
        "          if not isinstance(rhyme_dict[rhyme_idx], list) and not isinstance(rhyme_dict[rhyme_idx], tuple):\n",
        "            rhyme_dict[rhyme_idx] = [rhyme_dict[rhyme_idx]]\n",
        "          # Now get some words\n",
        "          if rhyme_idx not in rhyme_dict:\n",
        "            pdb.set_trace()\n",
        "          end_targets = [pick_rhyme(w, perfect=not use_near_rhymes, context=context, history=rhyme_history) for w in rhyme_dict[rhyme_idx]]\n",
        "        # There could be more than one possible rhyme target\n",
        "        ### GENERATE #####\n",
        "        for end_target in end_targets:\n",
        "          print(\"RHYME TARGET\", end_target)\n",
        "          # Check to see if the end_target already fills up all of our syllable length\n",
        "          end_target_syllables = get_syllables_for_line(end_target)\n",
        "          if end_target_syllables < target_syllables:\n",
        "            # Generate new lines\n",
        "            meet_target_syllables, too_short = generate_rhyme_line(end_target, context, target_syllables, terminate_line=terminate_segment)\n",
        "            goods = goods + meet_target_syllables\n",
        "            shorts = merge_dicts(shorts, too_short)\n",
        "          else:\n",
        "            # Just copy the end target because that is all the syllables we need\n",
        "            goods.append(end_target)\n",
        "      else:\n",
        "        ### NON-RHYME SEGMENT ###############################\n",
        "        # We are generating a new line unconstrained\n",
        "        # Generate new line\n",
        "        if terminate_segment:\n",
        "          # We've determined this segment must be terminal.\n",
        "          match_target_syllables1, shorts = generate_terminal_non_rhyme_line(context, target_syllables)\n",
        "          goods = match_target_syllables1\n",
        "        else:\n",
        "          # Segment does not need to be terminal\n",
        "          match_target_syllables1, _ = generate_non_rhyme_line(context, target_syllables)\n",
        "          match_target_syllables2 =  []\n",
        "          if is_end_segment:\n",
        "            match_target_syllables2, shorts = generate_terminal_non_rhyme_line(context, target_syllables)\n",
        "          goods = match_target_syllables1 + match_target_syllables2\n",
        "      ### PICK BEST CANDIDATE ############################################\n",
        "      results = []\n",
        "      if len(goods) > 0:\n",
        "        results = goods\n",
        "      else:\n",
        "        longest_key = max(list(shorts.keys()))\n",
        "        results = shorts[longest_key]\n",
        "      best_continuation, score = pick_best_line(results, context, history=segments, interactive=interactive)\n",
        "      new_syllables = get_syllables_for_line(best_continuation)\n",
        "      ### UPDATE RHYME DICTIONARY AND RHYME HISTORY ####################################\n",
        "      # If we are using near rhymes, store the near rhyme as possible target for future lines\n",
        "      if not isinstance(rhyme_idx, str) and rhyme_idx >=0:\n",
        "        # get end word\n",
        "        continuation = ''.join(list(filter(lambda c: c in STR_LETTERS or c == BLANK, best_continuation)))\n",
        "        split_continuation = continuation.split(BLANK)\n",
        "        end_word = split_continuation[-1]\n",
        "        if end_word[-1] in PUNCTUATION:\n",
        "          end_word = end_word[:-1]\n",
        "        # Make a new entry in rhyme_dict\n",
        "        if (rhyme_idx not in rhyme_dict) or (rhyme_idx in rhyme_dict and use_near_rhymes):\n",
        "          if rhyme_idx not in rhyme_dict:\n",
        "            rhyme_dict[rhyme_idx] = []\n",
        "          rhyme_dict[rhyme_idx].append(end_word)\n",
        "        # Add to rhyme history\n",
        "        rhyme_history.append(end_word)\n",
        "      ### DO SOME FIXING #################################################\n",
        "      # Fill in if we don't have enough syllables\n",
        "      if new_syllables < target_syllables:\n",
        "        rest = add_filler(best_continuation, target_syllables - new_syllables, 0)\n",
        "      ### SET UP CONTEXT FOR NEXT ITERATION ##########################\n",
        "      context = merge_lines(context, best_continuation)\n",
        "      print('CONTEXT', context)\n",
        "      current_line.append(best_continuation)\n",
        "      segments.append(best_continuation)\n",
        "      # get ready for the next iteration\n",
        "      context = crop_line(context)\n",
        "      previous_is_end_segment = is_end_segment\n",
        "    ### DONE WITH LINE #################################################\n",
        "    # put the current line together\n",
        "    lines.append(BLANK.join(fix_final_lines_punctuation(current_line)))\n",
        "  ### DONE WITH SPEC ###################################################\n",
        "  #lines = fix_final_lines_punctuation(lines)\n",
        "  lines = post_process_lines(lines, post)\n",
        "  lines = fix_final_lines_capitalization(lines)\n",
        "  return lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I3FhY_h1RL0"
      },
      "source": [
        "## GUI Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEmJVcpX1WeY"
      },
      "source": [
        "Extract the syllable and rhyme scheme from given lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMMAjAAs1Sw9"
      },
      "source": [
        "def extract_scheme(lines, use_near_rhymes = True):\n",
        "  syllables = []\n",
        "  ends = []\n",
        "  rhyme_lines = {}\n",
        "  # Compute syllables per line\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    num_syl = get_syllables_for_line(line)\n",
        "    syllables.append(num_syl)\n",
        "    ends.append(line[-1] == '.')\n",
        "  # Figure out which lines rhyme, piecewise\n",
        "  for i, line1 in enumerate(lines):\n",
        "    line1 = line1.strip()\n",
        "    words1 = line1.split()\n",
        "    last1 = words1[-1]\n",
        "    for j, line2 in enumerate(lines):\n",
        "      if i != j:\n",
        "        line2 = line2.strip()\n",
        "        words2 = line2.split()\n",
        "        last2 = words2[-1]\n",
        "        if perfect_rhyme(last1, last2) or (use_near_rhymes and near_rhyme(last1, last2)):\n",
        "          if i not in rhyme_lines:\n",
        "            rhyme_lines[i] = []\n",
        "          if j not in rhyme_lines:\n",
        "            rhyme_lines[j] = []\n",
        "          if j not in rhyme_lines[i]:\n",
        "            rhyme_lines[i].append(j)\n",
        "          if i not in rhyme_lines[j]:\n",
        "            rhyme_lines[j].append(i)\n",
        "  # Gather up all rhyming lines\n",
        "  for l1 in list(rhyme_lines.keys()):\n",
        "    for l2 in rhyme_lines[l1]:\n",
        "      for l3 in list(rhyme_lines[l2]):\n",
        "        if l3 not in rhyme_lines[l1]:\n",
        "          rhyme_lines[l1].append(l3)\n",
        "  # reduce to unique sets\n",
        "  rhyme_sets = []\n",
        "  for l in list(rhyme_lines.values()):\n",
        "    rhyme_set = set(l)\n",
        "    if rhyme_set not in rhyme_sets:\n",
        "      rhyme_sets.append(rhyme_set) \n",
        "  # Assign rhyme indexes to lines\n",
        "  rhyme_idxs = {}\n",
        "  for n, s in enumerate(rhyme_sets):\n",
        "    for l in s:\n",
        "      rhyme_idxs[l] = n\n",
        "  # Build schema\n",
        "  schema = []\n",
        "  for n in range(len(lines)):\n",
        "    segment = None\n",
        "    syl_num = syllables[n]\n",
        "    rhyme_idx = rhyme_idxs[n] if n in rhyme_idxs else -1\n",
        "    if ends[n]:\n",
        "      segment = (syl_num, rhyme_idx, ':end')\n",
        "    else:\n",
        "      segment = (syl_num, rhyme_idx)\n",
        "    schema.append(segment)\n",
        "  return schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x8OYI9S1bcn"
      },
      "source": [
        "Callback hooks from javascript/html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i9UMh_m1do7"
      },
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "def scheme_to_text(scheme):\n",
        "  text = ''\n",
        "  for i, x in enumerate(scheme):\n",
        "    text = text + str(x)\n",
        "    if i != len(scheme)-1:\n",
        "      text = text + ','\n",
        "    text = text + '\\n'\n",
        "  return text\n",
        "\n",
        "def python_run_hook(prompt, text, use_near_rhymes = False, recontextualize = False, interactive = False):\n",
        "  text = text.strip()\n",
        "  lines = text.split('\\n')\n",
        "  prompt = prompt.strip()\n",
        "  scheme = None\n",
        "  if text[0] == '[' or text[0] == '(':\n",
        "    if text[0] == '(':\n",
        "      text = '[' + text + ']'\n",
        "    try:\n",
        "        scheme = eval(text)\n",
        "    except:\n",
        "      print(\"Could not evaluate scheme text.\")\n",
        "  else:\n",
        "    scheme = extract_scheme(lines, use_near_rhymes)\n",
        "  if scheme is not None:\n",
        "    print(\"Syllable and rhyme scheme:\")\n",
        "    print(scheme_to_text(scheme))\n",
        "    lines = run(prompt, scheme, {}, recontextualize=recontextualize, use_near_rhymes=use_near_rhymes, post=[], interactive=interactive)\n",
        "    pretty_print(lines)\n",
        "    save_phone_cache(PHONE_CACHE, PHONE_CACHE_PATH)\n",
        "    return IPython.display.JSON({'result': 'true'})\n",
        "  else:\n",
        "    return IPython.display.JSON({'result': 'false'})\n",
        "\n",
        "def python_parse_hook(text, use_near_rhymes = False):\n",
        "  text = text.strip()\n",
        "  lines = text.split('\\n')\n",
        "  scheme = None\n",
        "  if text[0] == '[' or text[0] == '(':\n",
        "    if text[0] == '(':\n",
        "      text = '[' + text + ']'\n",
        "    try:\n",
        "        scheme = eval(text)\n",
        "    except:\n",
        "      print(\"Could not evaluate scheme text.\")\n",
        "  else:\n",
        "    scheme = extract_scheme(lines, use_near_rhymes)\n",
        "  if scheme is not None:\n",
        "    return IPython.display.JSON({'result': scheme_to_text(scheme)})\n",
        "  else:\n",
        "    return IPython.display.JSON({'result': text})\n",
        "\n",
        "output.register_callback('notebook.python_run_hook', python_run_hook)\n",
        "output.register_callback('notebook.python_parse_hook', python_parse_hook)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk7rxv50APc"
      },
      "source": [
        "# Basic User Interface\n",
        "\n",
        "You can launch the system by entering: \n",
        "\n",
        "1. **Context prompt:** a short phrase about the topic of the lyrics.\n",
        "2. **Lyrics:** Cut and paste lyrics from an existing song with each line on a separate line. (If a line ends with a period, the system will also end the line with a period.)\n",
        "3. Press \"Parse and Run!\" button\n",
        "\n",
        "The system will reverse engineer the syllable pattern and the rhyme scheme (only looking for the last words of each line).\n",
        "\n",
        "You can choose whether to use near rhymes (if unchecked, the system will only use perfect rhymes).\n",
        "\n",
        "You an choose to recontextualize. The system will attempt to reintroduce the context phrase throughout to keep the lyrics on topic. No guarantees the generator will stay on topic.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_k0TBRv0akQ"
      },
      "source": [
        "IPython.display.HTML(filename='/content/weirdai/gui.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ76XNmGziTT"
      },
      "source": [
        "# Advanced User Mode\n",
        "\n",
        "You may find you have more ability to control what the generator does with advanced mode. You will need to modify and execute some python code to do so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EL6v5mdGZ-t"
      },
      "source": [
        "## Rhyme Scheme\n",
        "\n",
        "A scheme indicates how many lines, how many syllables per line, and which lines rhyme with each other.\n",
        "\n",
        "A scheme is a list of tuples where each tuple is the number of syllables in a segment and an index (number)\n",
        "indicating which other segments to rhyme with.\n",
        "For example ```[(5, 1), (6, 2), (7, 1), (8, 2)]``` would indicate four segments, where the first and third segments rhyme. The first segment has five syllables, the second segment has six syllables, and so on.\n",
        "\n",
        "Instead of a rhyme index, you can also provide a word or phrase that the segment should end with.\n",
        "The number of syllables for that segments should be greater than or equal to the number of syllables\n",
        "in the word or phrase. For example, ```[(5, 1), (6, 1), (7, 'dead')]``` would make sure the last word of the last\n",
        "segment ended in the word ```'dead'```. \n",
        "Likewise ```[(3, 'hello world'), (6, 1), (7, 1)]``` would start with the entire first \n",
        "segment being 'hello world'. \n",
        "\n",
        "For most situations, a segment would correspond to a complete line on lyrics. But in some cases you want multiple segments per line, so as to have rhymes within a line. In this case, you can wrap tuples in a list, such as ```[(6, 1), [(4, 2), (4, 2), (4, 1)], (7, 1)]```, which would have three lines consisting of 6 syllables, 4+4+4=12 syllables, and 7 syllables, respectively. Each line would end in a rhyming word, but the second line would have two words that rhyme in the middle.\n",
        "\n",
        "The rhyme index is looked up in the rhyme dictionary for the song (```rhyme_dict```). This is not the *near rhyme dictionary* which is a list of all words known to nearly rhyme. The rhyme dictionary for the song is a list of words that the system should try to rhyme with when it gets to the end of a segment. If there is no entry in the dictionary, the system will generate forward unconstrained the appropriate number of syllables and the last word will be added to the ```rhyme_dict```. If the rhyme index is in the dictionary, then the system will pick a word that rhymes to end the segment in. If near rhyming is used, then near rhymes are added to the dictionary.In the ```rhyme_dict``` each rhyme index is associated with a list of words to rhyme with.\n",
        "\n",
        "The number of syllables for a segment can be indicated with ```-1``` if the rhyme index for the segment has been replaced by a string. The ```-1``` tells the system to use however many syllables are in the string.\n",
        "\n",
        "If the rhyme index for a segment is given as ```-1``` this tells the system to not try to rhyme with this segment.\n",
        "\n",
        "A segment can take a third parameter, special instructions on how to handle the segment, ```(syllables, rhyme_index, command)```. Currently the only command is ```\":end\"``` which tells the system that this segement should end in a period. \n",
        "\n",
        "There is a post-processing step to add text to the beginning or ending of lines. It is useful for \"ooh\"s and other things that one might not want the neural language models to pick up on and include in the generation if they are given instead of rhyme indices. Post-processing is specified with a special dictionary (```post```) where each key is a number indicating the line number to be post-processed. The value is a tuple ```(pre, post)``` where ```pre``` is a string to add to the beginning of the line and ```post``` is a string to add to the end of the line. Either can be ```None```.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ4boOSXGX6u"
      },
      "source": [
        "rhyme_dict = {}   # Rhyming lines are indicated by number. What word should certain lines rhyme with?\n",
        "post = {}         # Post-processing scheme\n",
        "\n",
        "# BEAT IT, MICHAEL JACKSON\n",
        "scheme = [\n",
        "#They told him don't you ever come around here\n",
        "          (11, 1), #0\n",
        "#Don't want to see your face, you better disappear\n",
        "          [(6, -1), (6, 1)], #1\n",
        "#The fire's in their eyes and their words are really clear\n",
        "          [(5, -1), (7, 1)], #2\n",
        "#So beat it, just beat it\n",
        "          (3, 2, ':end'), #3\n",
        "#You better run, you better do what you can\n",
        "          (11, 3), #4\n",
        "#Don't want to see no blood, don't be a macho man\n",
        "          [(6, -1), (6, 3)], #5\n",
        "#You want to be tough, better do what you can\n",
        "          [(5, -1), (6, 3)], #6\n",
        "#So beat it, but you want to be bad\n",
        "          [(3, 2), (6, -1)], #7\n",
        "#Just beat it, beat it, beat it, beat it\n",
        "          (3, 2, ':end'), #8\n",
        "#No one wants to be defeated\n",
        "          (8, 2), #9\n",
        "#Showin' how funky and strong is your fight\n",
        "          (10, 4), #10\n",
        "#It doesn't matter who's wrong or right\n",
        "          (9, 4), #11\n",
        "#Just beat it, beat it\n",
        "          (3, 2), #12\n",
        "#Just beat it, beat it\n",
        "          (3, 2), #13\n",
        "#Just beat it, beat it\n",
        "          (3, 2), #14\n",
        "#Just beat it, beat it\n",
        "          (3, 2, ':end') #15\n",
        "]\n",
        "#rhyme_dict[1] = ['here']\n",
        "post[3] = (None, ' just {repeat 2}.')\n",
        "post[8] = (None, ' ({repeat 2}), {repeat 2}, ({repeat 2})')\n",
        "post[12] = (None, ' ({repeat 2})')\n",
        "post[13] = (None, ' ({repeat 2})')\n",
        "post[14] = (None, ' ({repeat 2})')\n",
        "post[15] = (None, ' ({repeat 2}), oooooh')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-OHTh5nvfcZ"
      },
      "source": [
        "## Run!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkgzP3nivio1"
      },
      "source": [
        "Set the context to bias the agent toward a particular topic or theme. This will be added to the beginning of the lyrics as hidden text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTTJMk1pMncs"
      },
      "source": [
        "context = \"My favorite food is tacos\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4AGPk57GjBv"
      },
      "source": [
        "**Run the generator.**\n",
        "\n",
        "Set ```use_near_rhymes=True``` to use near rhymes or set to ```False``` to use perfect rhymes.\n",
        "\n",
        "Set ```recontextualize=True``` to try to force the generator to attend to the original context more often. This causes the original context to be added after every sentence break to try to steer the generators back to the context. It won't show up in the final output.\n",
        "\n",
        "Set ```interactive=True``` to manually select candidates for each segment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0InlSRUVEoIJ"
      },
      "source": [
        "RUN_ADVANCED_MODE = False #@param {type:\"boolean\"}\n",
        "use_near_rhymes = False #@param {type:\"boolean\"}\n",
        "recontextualize = False #@param {type:\"boolean\"}\n",
        "interactive = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Run the generator\n",
        "if RUN_ADVANCED_MODE:\n",
        "  lines = run(context, scheme, rhyme_dict, \n",
        "              use_near_rhymes=use_near_rhymes, \n",
        "              post=post, \n",
        "              recontextualize=recontextualize, \n",
        "              interactive=interactive)\n",
        "  # Print the outcome\n",
        "  pretty_print(lines)\n",
        "  # Save the phonentics cache\n",
        "  save_phone_cache(PHONE_CACHE, PHONE_CACHE_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGQ_wrxeZ2sr"
      },
      "source": [
        "# Generate Karaoke Video\n",
        "\n",
        "To do this you need an mp3 or mp4 file of music without vocals. A good way to do this is to find an existing karaoke video on YouTube. \n",
        "\n",
        "You will also need to know how many seconds each line of lyrics takes, down to at least the tenth of a second (but probably down to a millisecond to get the timing aligned)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDpxL_XpKCiD"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HuCnyaOVf7O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "0aa32dd6-116c-4d46-d2e4-68e697edf402"
      },
      "source": [
        "# The original pytube3 hasn't been updated since YouTube updated their API. \n",
        "# The unofficial clone below has the fix \n",
        "#!pip install pytube3\n",
        "#!pip install --upgrade git+https://gitlab.com/obuilds/public/pytube@ob-v1\n",
        "!git clone https://github.com/nficano/pytube.git\n",
        "!python -m pip install pytube"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://gitlab.com/obuilds/public/pytube@ob-v1\n",
            "  Cloning https://gitlab.com/obuilds/public/pytube (to revision ob-v1) to /tmp/pip-req-build-4587k60m\n",
            "  Running command git clone -q https://gitlab.com/obuilds/public/pytube /tmp/pip-req-build-4587k60m\n",
            "  Running command git checkout -q 10c57109f87fe864d8f38bbc8d76941e695de93a\n",
            "Requirement already satisfied, skipping upgrade: typing_extensions in /usr/local/lib/python3.6/dist-packages (from pytube3==9.6.4) (3.7.4.3)\n",
            "Building wheels for collected packages: pytube3\n",
            "  Building wheel for pytube3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytube3: filename=pytube3-9.6.4-cp36-none-any.whl size=38550 sha256=807a93899c470d8227f09bb41f5fb2eecf33d6beed3ad4c3b324f9d38ae119ca\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-72kz5gyz/wheels/94/1b/9d/b812d655a1f84514e2b91fc9c3d023b35c5067906b396e153e\n",
            "Successfully built pytube3\n",
            "Installing collected packages: pytube3\n",
            "Successfully installed pytube3-9.6.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMK0vlWWZ5XB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "305735f5-c7de-4ad6-dca2-f5d237e14a13"
      },
      "source": [
        "import os\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import moviepy.editor as mpe\n",
        "from pytube import YouTube"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3211264/45929032 bytes (7.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7471104/45929032 bytes (16.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11689984/45929032 bytes (25.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15884288/45929032 bytes (34.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19996672/45929032 bytes (43.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24174592/45929032 bytes (52.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28377088/45929032 bytes (61.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32563200/45929032 bytes (70.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36610048/45929032 bytes (79.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40673280/45929032 bytes (88.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44892160/45929032 bytes (97.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfTgvG7cRuDe"
      },
      "source": [
        "## Video Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Hh19RMKN6r"
      },
      "source": [
        "Need to set the font"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU6wKOZeKSwO"
      },
      "source": [
        "# Generate a list of fonts available on the OS\n",
        "#!fc-list\n",
        "# Set the font\n",
        "FONT = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\", size=24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ohyA9NKj_O"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2iVw5Z5KfTF"
      },
      "source": [
        "TEXT_OFFSET = (10, 10)\n",
        "EPSILON = 0.01\n",
        "\n",
        "def secs_to_millis(secs):\n",
        "  return secs*1000\n",
        "\n",
        "def convert(gif, mp4):\n",
        "    os.system(\"ffmpeg -f gif -i \" + gif + \" \" + mp4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsO_YvqPVXDy"
      },
      "source": [
        "Download from YouTube"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duJpnwdbVZcW"
      },
      "source": [
        "REMOVE_FROM_FILENAME = ['.', ',', '*']\n",
        "\n",
        "def download_youtube(url):\n",
        "  yt = YouTube(url)\n",
        "  title = yt.title\n",
        "  stream = yt.streams.filter(file_extension='mp4').first() #only_audio=True\n",
        "  stream.download()\n",
        "  return ''.join(c for c in title if not c in REMOVE_FROM_FILENAME) + '.mp4' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnNzezimKl7Q"
      },
      "source": [
        "Frame creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V268y6b5KlO5"
      },
      "source": [
        " def create_frame(lines, highlight, fnt, background = \"black\", text_color = 'white', highlight_color = 'yellow', frame_wh = (1024, 768), text_offset_xy = (0, 0)):\n",
        "    text_x, text_y = text_offset_xy\n",
        "    spacing = frame_wh[1] / len(lines)\n",
        "    img = Image.new('RGB', frame_wh, background)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    # draw.ellipse takes a 4-tuple (x0, y0, x1, y1) where (x0, y0) is the top-left bound of the box\n",
        "    # and (x1, y1) is the lower-right bound of the box.\n",
        "    for i, line in enumerate(lines):\n",
        "      color = text_color if i != highlight else highlight_color\n",
        "      draw.text((text_x, text_y + (i * spacing)), line, font = fnt, fill=color)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QMwILfjR7KC"
      },
      "source": [
        "def make_video_by_durations(filename, audio_file_or_youtube, \n",
        "                            lines, start_time, durations, \n",
        "                            music_artist, music_title, attribution = 'Weird A.I. Yankovic', \n",
        "                            new_title = None,\n",
        "                            frame_wh = (1024, 768),\n",
        "                            for_twitter = False):\n",
        "  # Copy durations\n",
        "  durations = list(map(lambda x:secs_to_millis(x), [start_time] + durations))\n",
        "  frames = []\n",
        "\n",
        "  # Make a title if none given\n",
        "  if new_title is None:\n",
        "    init_line = lines[0].split()\n",
        "    new_title = BLANK.join(init_line[0:min(4, len(init_line))])\n",
        "\n",
        "  # Make title frame\n",
        "  music_by = 'Music by: ' + music_artist + ' (\"' + music_title + '\")'\n",
        "  lyrics_by = 'Lyrics by: ' + attribution\n",
        "  title_lines = [new_title, music_by, lyrics_by]\n",
        "  temp_file_gif = filename + '.TEMP_GIF.gif'\n",
        "  temp_file_mp4 = filename + '.TEMP_VIDEO_MP4.mp4'\n",
        "  title_frame = create_frame(title_lines, 0, FONT, frame_wh=frame_wh, text_offset_xy=TEXT_OFFSET)\n",
        "  frames.append(title_frame)\n",
        "\n",
        "  # If we can, get the lyrics up early\n",
        "  if start_time > 5.0:\n",
        "    title_duration = 5.0\n",
        "    interstitial_duration = start_time - 5.0\n",
        "    if start_time > 10.0:\n",
        "      title_duration = 10.0\n",
        "      interstitial_duration = start_time - 10.0\n",
        "    durations = [secs_to_millis(title_duration), secs_to_millis(interstitial_duration)] + durations[1:]\n",
        "    preview_frame = create_frame(lines, -1, FONT, frame_wh=frame_wh, text_offset_xy=TEXT_OFFSET)\n",
        "    frames.append(preview_frame)\n",
        "\n",
        "  # Make rest of frames\n",
        "  for i, line in enumerate(lines):\n",
        "      new_frame = create_frame(lines, i, FONT, frame_wh=frame_wh, text_offset_xy=TEXT_OFFSET)\n",
        "      frames.append(new_frame)\n",
        "\n",
        "  # End lyrics frame\n",
        "  durations.append(secs_to_millis(EPSILON))\n",
        "  end_frame = create_frame(lines, -1, FONT, frame_wh=frame_wh, text_offset_xy=TEXT_OFFSET)\n",
        "  frames.append(end_frame)\n",
        "\n",
        "  # Save into a GIF file \n",
        "  frames[0].save(temp_file_gif, format='GIF',\n",
        "                append_images=frames[1:], save_all=True, duration=durations)\n",
        "  # Convert to MP4\n",
        "  convert(temp_file_gif, temp_file_mp4)\n",
        "  # Load mp4\n",
        "  my_clip = mpe.VideoFileClip(temp_file_mp4)\n",
        "  # Load audio (mp3 or mp4)\n",
        "  audio_file = audio_file_or_youtube\n",
        "  if 'https://www.youtube.com' in audio_file_or_youtube or 'https://youtu' in audio_file_or_youtube:\n",
        "    audio_file = download_youtube(audio_file_or_youtube)\n",
        "  # Sometimes there are naming problems with the youtube download\n",
        "  while not os.path.exists(audio_file):\n",
        "    audio_file = input(\"File not found. Please enter the name of the file: \")\n",
        "  # Get audio\n",
        "  audio_clip = mpe.AudioFileClip(audio_file)\n",
        "  # For Twitter\n",
        "  if for_twitter:\n",
        "    audio_clip = audio_clip.subclip(0, 120)\n",
        "  # Add audio to video\n",
        "  my_clip = my_clip.set_audio(audio_clip)\n",
        "  # Write mp4\n",
        "  my_clip.write_videofile(filename, \n",
        "                        audio=True,\n",
        "                        codec='libx264', \n",
        "                        audio_codec='aac', \n",
        "                        temp_audiofile='temp_audio.m4a', ) # default codec: 'libx264', 24 fps\n",
        "  # Clean up\n",
        "  os.remove(temp_file_gif)\n",
        "  os.remove(temp_file_mp4)\n",
        "\n",
        "def make_video_by_line_timing(filename, audio_file_or_youtube, \n",
        "                              lines, timing, total_duration, \n",
        "                              music_artist, music_title, attribution = 'Weird A.I. Yankovic',\n",
        "                              new_title = None, \n",
        "                              frame_wh = (1024, 768),\n",
        "                              for_twitter = False):\n",
        "  counter = timing[0]\n",
        "  durations = [counter]\n",
        "  for time in timing[1:]:\n",
        "    dur = time - counter\n",
        "    durations.append(dur)\n",
        "    counter = time\n",
        "  durations.append(total_duration - counter)\n",
        "  make_video_by_durations(filename, audio_file_or_youtube, lines, durations[0], durations[1:], music_artist, music_title, attribution, new_title, frame_wh, for_twitter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22iCwUrzR88y"
      },
      "source": [
        "## Run the Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y28iGiatKyrm"
      },
      "source": [
        "There are two ways to make the clip. \n",
        "\n",
        "1. If you know the duration of each line of lyrics, use ```make_video_by_durations()```. You will also need to set the start time of the first line so that lyrics match the music.\n",
        "\n",
        "2. If you know the start time of each line, use ```make_video_by_line_timing()```. You will also need to know the total duration of the lyrics.\n",
        "\n",
        "If you want a video that you can post to Twitter, it must be less than 2 minutes; setting ```for_twitter=True``` will clip the video. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4qCoETNKtSy"
      },
      "source": [
        "# BEAT IT, MICHAEL JACKSON\n",
        "# Timing works for https://www.youtube.com/watch?v=cuSi8yR9rKk\n",
        "start_time = 43.2\n",
        "line_durations = [3.0, 3.3, 3.4, 4.2, 2.8, 3.6, 3.3, 3.5, 3.7, 3.5, 3.4, 3.3, 1.9, 1.9, 1.4, 2.3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iudqml6QOobE"
      },
      "source": [
        "To run the karaoke video generation, set ```MAKE_KARAOKE_VIDEO=True``` and fill in the required values using the form to the right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdjlmp85MtL6"
      },
      "source": [
        "MAKE_KARAOKE_VIDEO = False #@param{type:\"boolean\"}\n",
        "save_filename = 'my_karaoke.mp4' #@param {type:\"string\"}\n",
        "title = 'Sassafrass' #@param {type:\"string\"}\n",
        "audio_file_or_youtube = 'https://www.youtube.com/watch?v=cuSi8yR9rKk' #@param {type:\"string\"}\n",
        "music_artist = 'Michael Jackson' #@param {type:\"string\"}\n",
        "original_music_title = 'Beat It' #@param {type:\"string\"}\n",
        "attribution = 'Weird A.I. Yankovic and its user' #@param {type:\"string\"}\n",
        "for_twitter = True #@param {type:\"boolean\"}\n",
        "\n",
        "if MAKE_KARAOKE_VIDEO: \n",
        "  make_video_by_durations(save_filename, audio_file_or_youtube, \n",
        "                          lines, start_time, line_durations, \n",
        "                          music_artist, original_music_title, \n",
        "                          new_title=title,\n",
        "                          attribution=attribution,\n",
        "                          for_twitter=for_twitter) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQVTdSuXCOcq"
      },
      "source": [
        "# License\n",
        "\n",
        "Copyright 2020 Mark Owen Riedl\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ]
}